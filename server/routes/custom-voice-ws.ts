import { WebSocketServer, WebSocket } from "ws";
import { Server } from 'http';
import { IncomingMessage } from 'http';
import { Socket } from 'net';
import { startDeepgramStream, DeepgramConnection } from "../services/deepgram-service";
import { generateTutorResponse, generateTutorResponseStreaming, StreamingCallbacks } from "../services/ai-service";
import { generateSpeech } from "../services/tts-service";
import { db } from "../db";
import { realtimeSessions, contentViolations, userSuspensions, documentChunks } from "@shared/schema";
import { eq, and, or, gte } from "drizzle-orm";
import { getTutorPersonality } from "../config/tutor-personalities";
import { moderateContent, shouldWarnUser, getModerationResponse } from "../services/content-moderation";
import { storage } from "../storage";
import { validateWsSession, rejectWsUpgrade } from '../middleware/ws-session-validator';
import { wsRateLimiter, getClientIp } from '../middleware/ws-rate-limiter';

// â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
// TIMING CONSTANTS (Nov 4, 2025): Comprehensive timing fix
// Prevents tutor from interrupting students mid-sentence
// Total delay target: 7-8 seconds from student stops â†’ tutor responds
// â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
const TIMING_CONFIG = {
  // Server-side delays before AI processing
  SERVER_DELAY_COMPLETE_THOUGHT: 1200,    // 1.2s for complete sentences (was 0ms)
  SERVER_DELAY_INCOMPLETE_THOUGHT: 2500,  // 2.5s for incomplete thoughts (e.g., "um", "I think")
  
  // Post-interruption buffer (when student interrupts tutor)
  POST_INTERRUPT_BUFFER: 2500,            // 2.5s extra wait after interruption
  
  // Combined with Deepgram settings:
  // - Deepgram endpointing: 3500ms (silence detection)
  // - Deepgram utterance_end_ms: 3500ms (finalization)
  // Total timing for complete thought: 3500ms (Deepgram) + 1200ms (server) = 4700ms
  // Total timing for incomplete thought: 3500ms (Deepgram) + 2500ms (server) = 6000ms
  // After interruption: Add +2500ms buffer = 6000-8500ms total
};

interface TranscriptEntry {
  speaker: 'tutor' | 'student';
  text: string;
  timestamp: string;
  messageId: string;
}

interface SessionState {
  sessionId: string;
  userId: string;
  studentName: string;
  ageGroup: string;
  language: string; // LANGUAGE: Tutoring language code (e.g., 'en', 'es', 'fr')
  detectedLanguage: string; // LANGUAGE: Auto-detected spoken language from Deepgram
  speechSpeed: number; // User's speech speed preference from settings
  systemInstruction: string;
  conversationHistory: Array<{ role: "user" | "assistant"; content: string }>;
  transcript: TranscriptEntry[];
  uploadedDocuments: string[];
  deepgramConnection: DeepgramConnection | null;
  isProcessing: boolean;
  transcriptQueue: string[]; // FIX #1: Queue for incoming transcripts
  sessionStartTime: number;
  lastPersisted: number;
  lastTranscript: string; // FIX #1A: Track last transcript to avoid duplicates
  violationCount: number; // Track content violations in this session
  isSessionEnded: boolean; // Flag to prevent further processing after termination
  isTutorSpeaking: boolean; // PACING FIX: Track if tutor is currently speaking
  lastAudioSentAt: number; // PACING FIX: Track when audio was last sent for interruption detection
  wasInterrupted: boolean; // TIMING FIX: Track if tutor was just interrupted (needs extra delay)
  lastInterruptionTime: number; // TIMING FIX: Track when last interruption occurred
  tutorAudioEnabled: boolean; // MODE: Whether tutor audio should play
  studentMicEnabled: boolean; // MODE: Whether student microphone is active
  lastActivityTime: number; // INACTIVITY: Track last user speech activity
  inactivityWarningSent: boolean; // INACTIVITY: Track if 4-minute warning has been sent
  inactivityTimerId: NodeJS.Timeout | null; // INACTIVITY: Timer for checking inactivity
  isReconnecting: boolean; // RECONNECT: Track if Deepgram reconnection is in progress (blocks audio)
}

// FIX #3: Incremental persistence helper
async function persistTranscript(sessionId: string, transcript: TranscriptEntry[]) {
  if (!sessionId || transcript.length === 0) return;
  
  try {
    // Type-safe persistence: transcript column expects JSONB array of {speaker, text, timestamp}
    await db.update(realtimeSessions)
      .set({
        transcript: transcript,
      })
      .where(eq(realtimeSessions.id, sessionId));
    console.log(`[Custom Voice] ğŸ’¾ Persisted ${transcript.length} transcript entries`);
  } catch (error) {
    console.error("[Custom Voice] âŒ Error persisting transcript:", error);
  }
}

// â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
// TIMING FIX (Nov 3, 2025): Incomplete thought detection
// Detect when students are likely still formulating their response
// â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
function isLikelyIncompleteThought(transcript: string): boolean {
  const text = transcript.trim().toLowerCase();
  const wordCount = text.split(/\s+/).length;
  
  // Very short responses are often incomplete ("yeah", "um", "I", "it")
  if (wordCount <= 2) {
    return true;
  }
  
  // Common incomplete sentence starters
  const incompleteStarters = [
    /^(yeah|uh|um|so|well|and|but|it|the|i)\s*$/i,
    /^(i think|i mean|it says|it basically|well i|so i)\s*$/i,
    /^(the|a|this|that)\s+\w+\s*$/i, // "the problem", "a question"
  ];
  
  for (const pattern of incompleteStarters) {
    if (pattern.test(text)) {
      return true;
    }
  }
  
  // Trailing conjunctions suggest more coming
  if (/\b(and|but|or|so|because|since|unless)\s*$/i.test(text)) {
    return true;
  }
  
  return false;
}

// Centralized session finalization helper (prevents double-processing and ensures consistency)
async function finalizeSession(
  state: SessionState,
  reason: 'normal' | 'disconnect' | 'error' | 'violation' | 'inactivity_timeout',
  errorMessage?: string
) {
  // Idempotent: skip if already finalized
  if (state.isSessionEnded) {
    console.log(`[Custom Voice] â„¹ï¸ Session already finalized, skipping (reason: ${reason})`);
    return;
  }

  // Mark as ended FIRST to prevent race conditions
  state.isSessionEnded = true;
  
  // CRITICAL: Clear inactivity timer to prevent duplicate finalization
  if (state.inactivityTimerId) {
    clearInterval(state.inactivityTimerId);
    state.inactivityTimerId = null;
    console.log(`[Finalize] ğŸ§¹ Cleared inactivity timer (reason: ${reason})`);
  }

  if (!state.sessionId) {
    console.warn('[Custom Voice] âš ï¸ No sessionId, skipping finalization');
    return;
  }

  try {
    // Calculate session duration
    const durationSeconds = Math.floor((Date.now() - state.sessionStartTime) / 1000);
    const durationMinutes = Math.max(1, Math.ceil(durationSeconds / 60));

    // Update database with complete session data
    const updateData: any = {
      transcript: state.transcript,
      endedAt: new Date(),
      status: 'ended',
      minutesUsed: durationMinutes,
      totalMessages: state.transcript.length,
    };

    if (errorMessage) {
      updateData.errorMessage = errorMessage;
    }

    await db.update(realtimeSessions)
      .set(updateData)
      .where(eq(realtimeSessions.id, state.sessionId));

    console.log(`[Custom Voice] ğŸ’¾ Session finalized (${reason}) - ${durationMinutes} minutes, ${state.transcript.length} messages`);

    // Deduct minutes from user balance
    if (state.userId && durationMinutes > 0) {
      const { deductMinutes } = await import('../services/voice-minutes');
      await deductMinutes(state.userId, durationMinutes);
      console.log(`[Custom Voice] âœ… Deducted ${durationMinutes} minutes from user ${state.userId}`);
    }
  } catch (error) {
    console.error(`[Custom Voice] âŒ Error finalizing session (${reason}):`, error);
  }
}

export function setupCustomVoiceWebSocket(server: Server) {
  // Use noServer mode for manual upgrade with session authentication
  const wss = new WebSocketServer({ noServer: true });

  console.log('[Custom Voice] WebSocket server initialized on /api/custom-voice-ws (noServer mode)');

  // Handle WebSocket upgrade with production-grade authentication
  server.on('upgrade', async (request: IncomingMessage, socket: Socket, head: Buffer) => {
    // Only handle /api/custom-voice-ws path (allow query strings)
    const url = request.url || '';
    if (!url.startsWith('/api/custom-voice-ws')) {
      // Not our path - destroy socket to prevent leaks
      socket.destroy();
      return;
    }

    console.log('[WebSocket] ğŸ” Validating upgrade request...');

    // Step 1: IP-based rate limiting (prevent DoS attacks)
    const clientIp = getClientIp(request);
    const rateLimitCheck = wsRateLimiter.canUpgrade(clientIp);
    
    if (!rateLimitCheck.allowed) {
      console.error(`[WebSocket] âŒ Rate limit exceeded for ${clientIp}:`, rateLimitCheck.reason);
      rejectWsUpgrade(socket, 429, rateLimitCheck.reason || 'Too many requests');
      return;
    }

    // Step 2: Session validation (no Express middleware reuse)
    const sessionSecret = process.env.SESSION_SECRET || 'development-session-secret-only';
    const validationResult = await validateWsSession(request, sessionSecret);
    
    if (!validationResult.valid) {
      console.error(`[WebSocket] âŒ Session validation failed:`, validationResult.error);
      rejectWsUpgrade(socket, validationResult.statusCode || 401, validationResult.error || 'Unauthorized');
      return;
    }

    const userId = validationResult.userId!;
    const sessionId = validationResult.sessionId!;
    console.log('[WebSocket] âœ… Session validated for user:', userId);

    // Step 3: Upgrade to WebSocket
    try {
      wss.handleUpgrade(request, socket, head, (ws) => {
        // Track connection for rate limiter (enforces concurrent connection limit)
        const trackResult = wsRateLimiter.trackConnection(clientIp);
        
        if (!trackResult.allowed) {
          console.error(`[WebSocket] âŒ Concurrent limit exceeded for ${clientIp}`);
          ws.close(1008, trackResult.reason || 'Too many concurrent connections');
          return;
        }
        
        // Attach authenticated userId to WebSocket
        (ws as any).authenticatedUserId = userId;
        (ws as any).sessionId = sessionId;
        (ws as any).clientIp = clientIp;
        
        console.log('[WebSocket] âœ… Connection tracked for user:', userId);
        
        // Release connection when socket closes
        ws.on('close', () => {
          wsRateLimiter.releaseConnection(clientIp);
          console.log(`[WebSocket] âœ… Connection released for ${clientIp}`);
        });
        
        wss.emit('connection', ws, request);
      });
    } catch (upgradeError) {
      console.error('[WebSocket] âŒ Upgrade error:', upgradeError);
      rejectWsUpgrade(socket, 500, 'Internal server error');
    }
  });

  wss.on("connection", (ws: WebSocket) => {
    // Get authenticated userId that was attached during upgrade
    const authenticatedUserId = (ws as any).authenticatedUserId as string;
    const user = (ws as any).user;
    
    console.log("[Custom Voice] ğŸ”Œ New authenticated connection for user:", authenticatedUserId);
    
    // FIX #2C: Turn-taking timeout for natural conversation flow
    let responseTimer: NodeJS.Timeout | null = null;
    
    // FIX (Dec 10, 2025): Track reconnection attempts to prevent infinite loops
    let reconnectAttempts = 0;
    
    const state: SessionState = {
      sessionId: "",
      userId: authenticatedUserId, // Use session-authenticated userId
      studentName: "",
      ageGroup: "default",
      language: "en", // LANGUAGE: Default to English, will be set from session
      detectedLanguage: "", // LANGUAGE: Auto-detected spoken language from Deepgram
      speechSpeed: 0.95, // Default speech speed, will be overridden by user preference
      systemInstruction: "",
      conversationHistory: [],
      transcript: [],
      uploadedDocuments: [],
      deepgramConnection: null,
      isProcessing: false,
      transcriptQueue: [], // FIX #1: Initialize queue
      sessionStartTime: Date.now(),
      lastPersisted: Date.now(),
      lastTranscript: "", // FIX #1A: Initialize duplicate tracker
      tutorAudioEnabled: true, // MODE: Default to audio enabled
      studentMicEnabled: true, // MODE: Default to mic enabled
      violationCount: 0, // Initialize violation counter
      isSessionEnded: false, // Initialize session termination flag
      isTutorSpeaking: false, // PACING FIX: Initialize tutor speaking state
      lastAudioSentAt: 0, // PACING FIX: Initialize audio timestamp
      wasInterrupted: false, // TIMING FIX: Initialize interruption flag
      lastInterruptionTime: 0, // TIMING FIX: Initialize interruption timestamp
      lastActivityTime: Date.now(), // INACTIVITY: Initialize to now
      inactivityWarningSent: false, // INACTIVITY: No warning sent yet
      inactivityTimerId: null, // INACTIVITY: Timer not started yet
      isReconnecting: false, // RECONNECT: Not reconnecting initially
    };

    // FIX #3: Auto-persist every 10 seconds
    const persistInterval = setInterval(async () => {
      if (state.sessionId && state.transcript.length > 0) {
        await persistTranscript(state.sessionId, state.transcript);
        state.lastPersisted = Date.now();
      }
    }, 10000);

    // INACTIVITY: Check for user inactivity every 30 seconds
    state.inactivityTimerId = setInterval(async () => {
      const inactiveTime = Date.now() - state.lastActivityTime;
      const inactiveMinutes = Math.floor(inactiveTime / 60000);
      const inactiveSeconds = Math.floor((inactiveTime % 60000) / 1000);
      
      console.log(`[Inactivity] â±ï¸ Check: ${inactiveMinutes}m ${inactiveSeconds}s since last activity`);
      
      // WARNING AT 4 MINUTES
      if (inactiveMinutes >= 4 && !state.inactivityWarningSent) {
        console.log('[Inactivity] â° 4 minutes inactive - sending warning');
        
        const warningMessage = "Hey, are you still there? I haven't heard from you in a while. If you're done learning for now, just say 'goodbye' or I'll automatically end our session in one minute.";
        
        // Add to transcript
        const warningEntry: TranscriptEntry = {
          speaker: "tutor",
          text: warningMessage,
          timestamp: new Date().toISOString(),
          messageId: crypto.randomUUID(),
        };
        state.transcript.push(warningEntry);
        
        // Send warning to frontend
        ws.send(JSON.stringify({
          type: 'transcript',
          speaker: 'tutor',
          text: warningMessage,
        }));
        
        // Generate speech for warning if audio is enabled
        if (state.tutorAudioEnabled) {
          try {
            const audioBuffer = await generateSpeech(warningMessage, state.ageGroup, state.speechSpeed);
            if (audioBuffer) {
              ws.send(JSON.stringify({
                type: 'audio',
                audio: audioBuffer.toString('base64'),
              }));
              console.log('[Inactivity] ğŸ”Š Warning audio sent');
            }
          } catch (audioError) {
            console.error('[Inactivity] âŒ Error generating warning audio:', audioError);
          }
        }
        
        state.inactivityWarningSent = true;
      }
      
      // AUTO-END AT 5 MINUTES
      if (inactiveMinutes >= 5) {
        console.log('[Inactivity] â° 5 minutes inactive - auto-ending session');
        
        const endMessage = "I haven't heard from you in 5 minutes, so I'm going to end our session now. Feel free to come back anytime you want to learn more!";
        
        // Add to transcript
        const endEntry: TranscriptEntry = {
          speaker: "tutor",
          text: endMessage,
          timestamp: new Date().toISOString(),
          messageId: crypto.randomUUID(),
        };
        state.transcript.push(endEntry);
        
        // Send final message to frontend
        ws.send(JSON.stringify({
          type: 'transcript',
          speaker: 'tutor',
          text: endMessage,
        }));
        
        // Generate speech for end message if audio is enabled
        if (state.tutorAudioEnabled) {
          try {
            const audioBuffer = await generateSpeech(endMessage, state.ageGroup, state.speechSpeed);
            if (audioBuffer) {
              ws.send(JSON.stringify({
                type: 'audio',
                audio: audioBuffer.toString('base64'),
              }));
              console.log('[Inactivity] ğŸ”Š End message audio sent');
            }
          } catch (audioError) {
            console.error('[Inactivity] âŒ Error generating end audio:', audioError);
          }
        }
        
        // Wait 5 seconds for message to play, then end session
        setTimeout(async () => {
          console.log('[Inactivity] ğŸ›‘ Auto-ending session due to inactivity');
          
          try {
            // Send session end notification to frontend
            ws.send(JSON.stringify({
              type: 'session_ended',
              reason: 'inactivity_timeout',
              message: 'Session ended due to inactivity'
            }));
            
            // Finalize session in database
            await finalizeSession(state, 'normal');
            
            // Close WebSocket
            ws.close(1000, 'Session ended due to inactivity');
            
            console.log('[Inactivity] âœ… Session ended successfully');
            
          } catch (error) {
            console.error('[Inactivity] âŒ Error ending session:', error);
            ws.close(1011, 'Error ending session');
          }
          
        }, 5000); // 5 second delay
        
        // Clear the interval to prevent multiple triggers
        if (state.inactivityTimerId) {
          clearInterval(state.inactivityTimerId);
          state.inactivityTimerId = null;
        }
      }
      
    }, 30000); // Check every 30 seconds

    console.log('[Inactivity] âœ… Checker started (checks every 30 seconds)');

    // FIX #1: Process queued transcripts sequentially
    async function processTranscriptQueue() {
      if (state.isProcessing || state.transcriptQueue.length === 0 || state.isSessionEnded) {
        return;
      }

      state.isProcessing = true;
      const transcript = state.transcriptQueue.shift()!;

      try {
        // Add to transcript log
        const transcriptEntry: TranscriptEntry = {
          speaker: "student",
          text: transcript,
          timestamp: new Date().toISOString(),
          messageId: crypto.randomUUID(),
        };
        state.transcript.push(transcriptEntry);

        // Send transcript to frontend
        ws.send(JSON.stringify({
          type: "transcript",
          speaker: "student",
          text: transcript,
        }));

        console.log(`[Custom Voice] ğŸ‘¤ ${state.studentName}: "${transcript}"`);

        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        // ğŸ›¡ï¸ CONTENT MODERATION - Check for inappropriate content
        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        console.log("[Custom Voice] ğŸ” Moderating content...");
        
        // Pass educational context to moderation
        const moderation = await moderateContent(transcript, {
          sessionType: 'tutoring',
          subject: 'general', // Subject from session init message
          gradeLevel: state.ageGroup,
          hasDocuments: state.uploadedDocuments && state.uploadedDocuments.length > 0
        });
        
        console.log("[Custom Voice] Moderation result:", {
          isAppropriate: moderation.isAppropriate,
          confidence: moderation.confidence,
          reason: moderation.reason
        });
        
        if (!moderation.isAppropriate) {
          console.log(`[Custom Voice] âš ï¸  Content flagged: ${moderation.violationType} (confidence: ${moderation.confidence})`);
          
          // Only take action on HIGH confidence violations (>0.85)
          // This prevents false positives from ending sessions
          if (moderation.confidence && moderation.confidence > 0.85) {
            console.log("[Custom Voice] âŒ High confidence violation - taking action");
            
            // Increment violation count
            state.violationCount++;
            const warningLevel = shouldWarnUser(state.violationCount - 1);
            
            // Get appropriate response based on warning level (should never be 'none' here)
            if (warningLevel === 'none') {
              console.error("[Custom Voice] âŒ Unexpected warning level 'none'");
              state.isProcessing = false; // CRITICAL FIX: Release processing lock
              return; // Skip if somehow 'none' is returned
            }
            
            const moderationResponse = getModerationResponse(warningLevel);
          
          // Log violation to database with FULL context (user message + AI warning response)
          await db.insert(contentViolations).values({
            userId: state.userId,
            sessionId: state.sessionId,
            violationType: moderation.violationType!,
            severity: moderation.severity,
            userMessage: transcript,
            aiResponse: moderationResponse, // Store the warning message for admin review
            confidence: moderation.confidence?.toString(),
            reviewStatus: 'pending',
            actionTaken: warningLevel === 'final' ? 'suspension' : 'warning',
          });
          
          // If final warning, suspend user and end session
          if (warningLevel === 'final') {
            console.log("[Custom Voice] ğŸš« Suspending user due to repeated violations");
            
            // Create suspension record (24 hour suspension)
            const suspendedUntil = new Date();
            suspendedUntil.setHours(suspendedUntil.getHours() + 24);
            
            await db.insert(userSuspensions).values({
              userId: state.userId,
              reason: `Repeated inappropriate content violations (${moderation.violationType})`,
              violationIds: [], // Could track specific violation IDs
              suspendedUntil: suspendedUntil,
              isPermanent: false,
              isActive: true,
            });
            
            // Send moderation response
            const aiResponse = moderationResponse;
            
            // Add to conversation history
            state.conversationHistory.push(
              { role: "user", content: transcript },
              { role: "assistant", content: aiResponse }
            );
            
            // Add to transcript
            const aiTranscriptEntry: TranscriptEntry = {
              speaker: "tutor",
              text: aiResponse,
              timestamp: new Date().toISOString(),
              messageId: crypto.randomUUID(),
            };
            state.transcript.push(aiTranscriptEntry);
            
            // Send response
            ws.send(JSON.stringify({
              type: "transcript",
              speaker: "tutor",
              text: aiResponse,
            }));
            
            // Generate and send speech
            const audioBuffer = await generateSpeech(aiResponse, state.ageGroup, state.speechSpeed);
            ws.send(JSON.stringify({
              type: "audio",
              data: audioBuffer.toString("base64"),
              mimeType: "audio/pcm;rate=16000"
            }));
            
            // Clear intervals before finalizing (inactivity timer cleared in finalizeSession)
            clearInterval(persistInterval);
            if (responseTimer) {
              clearTimeout(responseTimer);
              responseTimer = null;
            }
            
            // Finalize session with violation reason
            await finalizeSession(state, 'violation', `Content violation: ${moderation.violationType}`);
            
            // Send session end notification
            ws.send(JSON.stringify({
              type: "session_ended",
              reason: "content_violation"
            }));
            
            // Close WebSocket
            state.isProcessing = false; // CRITICAL FIX: Release processing lock
            setTimeout(() => ws.close(), 2000); // Give time for audio to play
            return;
          } else {
            // Send warning (1st or 2nd)
            console.log(`[Custom Voice] âš ï¸  Sending ${warningLevel} warning to user`);
            const aiResponse = moderationResponse;
            
            // Add to conversation history
            state.conversationHistory.push(
              { role: "user", content: transcript },
              { role: "assistant", content: aiResponse }
            );
            
            // Add to transcript
            const aiTranscriptEntry: TranscriptEntry = {
              speaker: "tutor",
              text: aiResponse,
              timestamp: new Date().toISOString(),
              messageId: crypto.randomUUID(),
            };
            state.transcript.push(aiTranscriptEntry);
            
            // Send response
            ws.send(JSON.stringify({
              type: "transcript",
              speaker: "tutor",
              text: aiResponse,
            }));
            
            // Generate and send speech
            const audioBuffer = await generateSpeech(aiResponse, state.ageGroup, state.speechSpeed);
            ws.send(JSON.stringify({
              type: "audio",
              data: audioBuffer.toString("base64"),
              mimeType: "audio/pcm;rate=16000"
            }));
            
            // Persist
            await persistTranscript(state.sessionId, state.transcript);
            state.isProcessing = false; // CRITICAL FIX: Release processing lock after warning
            
            // Process next queued item if any
            if (state.transcriptQueue.length > 0 && !state.isSessionEnded) {
              setImmediate(() => processTranscriptQueue());
            }
            return; // Don't continue to normal AI processing
          }
          } else {
            // Low confidence flag - log but proceed with educational conversation
            console.warn("[Custom Voice] âš ï¸ Low confidence flag - proceeding with educational context:", {
              message: transcript,
              confidence: moderation.confidence,
              reason: moderation.reason
            });
            // Continue to normal AI processing below
          }
        }
        
        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        // âœ… Content passed moderation - Continue normal processing
        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        // â±ï¸ LATENCY TIMING: Start pipeline timing
        const pipelineStart = Date.now();
        console.log(`[Custom Voice] â±ï¸ PIPELINE START at ${new Date().toISOString()}`);
        
        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        // TIMING OPTIMIZATION (Dec 5, 2025): Reduced delays for faster response
        // Previous delays were too long (1200-2500ms), now reduced significantly
        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        // Calculate appropriate delay based on context (REDUCED for faster response)
        let responseDelay = 300; // Reduced from 1200ms to 300ms for complete thoughts
        
        // Check if this was likely an incomplete thought
        if (isLikelyIncompleteThought(transcript)) {
          responseDelay = 800; // Reduced from 2500ms to 800ms for incomplete thoughts
          console.log(`[Custom Voice] â±ï¸ Detected incomplete thought - using delay (${responseDelay}ms)`);
        } else {
          console.log(`[Custom Voice] â±ï¸ Complete thought detected - using minimal delay (${responseDelay}ms)`);
        }
        
        // Add extra buffer if student just interrupted tutor (reduced)
        if (state.wasInterrupted) {
          const timeSinceInterrupt = Date.now() - state.lastInterruptionTime;
          if (timeSinceInterrupt < 10000) { // Within 10 seconds
            const extraBuffer = 500; // Reduced from 2500ms to 500ms
            console.log(`[Custom Voice] ğŸ›‘ Post-interruption buffer: +${extraBuffer}ms (interrupted ${timeSinceInterrupt}ms ago)`);
            responseDelay += extraBuffer;
          }
          state.wasInterrupted = false; // Clear flag after applying
        }
        
        console.log(`[Custom Voice] â³ Pre-response delay: ${responseDelay}ms...`);
        await new Promise(resolve => setTimeout(resolve, responseDelay));
        console.log(`[Custom Voice] â±ï¸ Delay done (+${Date.now() - pipelineStart}ms), calling Claude...`);
        
        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        // Generate AI response (voice input) - STREAMING for lower latency
        // LANGUAGE AUTO-DETECT: Use detected language if available, fall back to selected
        const responseLanguage = state.detectedLanguage || state.language;
        console.log(`[Custom Voice] ğŸŒ Generating STREAMING response in: ${responseLanguage}`);
        
        // â±ï¸ LATENCY TIMING: Track streaming response
        const claudeStart = Date.now();
        let firstSentenceMs = 0;
        let totalTtsMs = 0;
        let totalAudioBytes = 0;
        let sentenceCount = 0;
        
        // Track turn for interruption detection
        state.isTutorSpeaking = true;
        const turnTimestamp = Date.now();
        state.lastAudioSentAt = turnTimestamp;
        
        // Use streaming with sentence-by-sentence TTS for minimal latency
        await new Promise<void>((resolve, reject) => {
          const callbacks: StreamingCallbacks = {
            onSentence: async (sentence: string) => {
              sentenceCount++;
              const sentenceStart = Date.now();
              
              if (sentenceCount === 1) {
                firstSentenceMs = sentenceStart - claudeStart;
                console.log(`[Custom Voice] â±ï¸ First sentence in ${firstSentenceMs}ms`);
                
                // Send full transcript placeholder for first sentence
                ws.send(JSON.stringify({
                  type: "transcript",
                  speaker: "tutor",
                  text: sentence,
                  isPartial: true,
                }));
              } else {
                // Update transcript with accumulated text
                ws.send(JSON.stringify({
                  type: "transcript_update",
                  speaker: "tutor",
                  text: sentence,
                }));
              }
              
              // Generate TTS for this sentence immediately
              if (state.tutorAudioEnabled) {
                const ttsStart = Date.now();
                try {
                  const audioBuffer = await generateSpeech(sentence, state.ageGroup, state.speechSpeed);
                  const ttsMs = Date.now() - ttsStart;
                  totalTtsMs += ttsMs;
                  totalAudioBytes += audioBuffer.length;
                  
                  console.log(`[Custom Voice] ğŸ”Š Sentence ${sentenceCount} TTS: ${ttsMs}ms, ${audioBuffer.length} bytes`);
                  
                  // Send audio chunk immediately
                  ws.send(JSON.stringify({
                    type: "audio",
                    data: audioBuffer.toString("base64"),
                    mimeType: "audio/pcm;rate=16000",
                    isChunk: true,
                    chunkIndex: sentenceCount,
                  }));
                } catch (ttsError) {
                  console.error(`[Custom Voice] âŒ TTS error for sentence ${sentenceCount}:`, ttsError);
                }
              }
            },
            
            onComplete: (fullText: string) => {
              const claudeMs = Date.now() - claudeStart;
              console.log(`[Custom Voice] â±ï¸ Streaming complete: ${claudeMs}ms total, ${sentenceCount} sentences`);
              console.log(`[Custom Voice] ğŸ¤– Tutor: "${fullText}"`);
              
              // Add to conversation history
              state.conversationHistory.push(
                { role: "user", content: transcript },
                { role: "assistant", content: fullText }
              );
              
              // Add AI response to transcript (internal state)
              const aiTranscriptEntry: TranscriptEntry = {
                speaker: "tutor",
                text: fullText,
                timestamp: new Date().toISOString(),
                messageId: crypto.randomUUID(),
              };
              state.transcript.push(aiTranscriptEntry);
              
              // Send final complete transcript
              ws.send(JSON.stringify({
                type: "transcript",
                speaker: "tutor",
                text: fullText,
                isComplete: true,
              }));
              
              // â±ï¸ LATENCY TIMING: Total pipeline time
              const totalPipelineMs = Date.now() - pipelineStart;
              console.log(`[Custom Voice] â±ï¸ PIPELINE COMPLETE (STREAMING): ${totalPipelineMs}ms total`);
              console.log(`[Custom Voice] â±ï¸ Breakdown: delay=${responseDelay}ms, firstSentence=${firstSentenceMs}ms, totalTTS=${totalTtsMs}ms, audio=${totalAudioBytes} bytes`);
              
              resolve();
            },
            
            onError: (error: Error) => {
              console.error("[Custom Voice] âŒ Streaming error:", error);
              reject(error);
            }
          };
          
          generateTutorResponseStreaming(
            state.conversationHistory,
            transcript,
            state.uploadedDocuments,
            callbacks,
            state.systemInstruction,
            "voice",
            responseLanguage
          ).catch(reject); // Ensure errors are properly propagated
        });

        console.log("[Custom Voice] ğŸ”Š Streaming response sent, waiting for user...");

        // FIX #3: Persist after each turn (before pause to avoid blocking)
        await persistTranscript(state.sessionId, state.transcript);

        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        // PACING FIX: Release isProcessing BEFORE pause to allow interruptions
        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        state.isProcessing = false;
        
        // Calculate approximate audio duration from total bytes (16kHz, 16-bit = 2 bytes/sample)
        const audioDuration = totalAudioBytes / (16000 * 2); // seconds
        const pauseMs = Math.max(2000, audioDuration * 1000 + 1500); // Audio duration + 1.5s buffer

        console.log(`[Custom Voice] â³ Pausing ${pauseMs}ms (audio: ${audioDuration.toFixed(1)}s + 1.5s buffer)...`);

        // Wait for audio to finish playing + give user time to think
        await new Promise(resolve => setTimeout(resolve, pauseMs));

        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        // PACING FIX: Only clear flag if this turn is still active (prevents race condition)
        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if (state.lastAudioSentAt === turnTimestamp) {
          console.log("[Custom Voice] âœ… Pause complete, ready for user input");
          state.isTutorSpeaking = false;
        } else {
          console.log("[Custom Voice] â„¹ï¸ Turn superseded by newer turn, keeping isTutorSpeaking");
        }
        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        // Process next queued item if any
        if (state.transcriptQueue.length > 0 && !state.isSessionEnded) {
          setImmediate(() => processTranscriptQueue());
        }

      } catch (error) {
        console.error("[Custom Voice] âŒ Error processing:", error);
        state.isTutorSpeaking = false; // Reset on error
        ws.send(JSON.stringify({ 
          type: "error", 
          error: error instanceof Error ? error.message : "Unknown error"
        }));
        
        // FIX #1: Process next item in queue even after error
        if (state.transcriptQueue.length > 0 && !state.isSessionEnded) {
          setImmediate(() => processTranscriptQueue());
        }
      } finally {
        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        // CRITICAL FIX (Dec 10, 2025): ALWAYS release processing lock
        // Ensures tutor never gets stuck silent due to unreleased locks
        // This is idempotent - safe to call even if already false
        // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        state.isProcessing = false;
      }
    }

    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    // FIX (Dec 10, 2025): Reconnect Deepgram function for auto-recovery
    // Properly tears down previous connection before creating new one
    // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    async function reconnectDeepgram(): Promise<DeepgramConnection> {
      // CRITICAL: Tear down existing connection to prevent listener/interval leaks
      if (state.deepgramConnection) {
        console.log("[Custom Voice] ğŸ§¹ Tearing down old Deepgram connection before reconnect");
        try {
          state.deepgramConnection.close();
        } catch (e) {
          // Ignore close errors on old connection
        }
        state.deepgramConnection = null;
      }
      
      const { getDeepgramLanguageCode } = await import("../services/deepgram-service");
      const deepgramLanguage = getDeepgramLanguageCode(state.language);
      
      // Shared transcript handler - same logic as original connection
      const handleTranscript = async (transcript: string, isFinal: boolean, detectedLanguage?: string) => {
        const spokenLang = detectedLanguage || state.language;
        console.log(`[Deepgram] ${isFinal ? 'âœ… FINAL' : 'â³ interim'}: "${transcript}" (reconnected, lang=${spokenLang})`);
        
        if (!state.userId) return;
        
        // BARGE-IN
        const timeSinceLastAudio = Date.now() - state.lastAudioSentAt;
        if (state.isTutorSpeaking && timeSinceLastAudio < 30000 && transcript.trim().length >= 2) {
          state.wasInterrupted = true;
          state.lastInterruptionTime = Date.now();
          ws.send(JSON.stringify({ type: "interrupt", message: "Student is speaking" }));
          state.isTutorSpeaking = false;
        }
        
        if (state.isTutorSpeaking && timeSinceLastAudio >= 30000) {
          state.isTutorSpeaking = false;
        }
        
        if (!isFinal) return;
        if (!transcript || transcript.trim().length < 3) return;
        
        state.lastActivityTime = Date.now();
        state.inactivityWarningSent = false;
        
        if (state.isProcessing) return;
        if (state.lastTranscript === transcript) return;
        
        state.lastTranscript = transcript;
        if (spokenLang) state.detectedLanguage = spokenLang;
        
        console.log(`[Custom Voice] âœ… Processing FINAL transcript (reconnected): "${transcript}"`);
        
        if (responseTimer) {
          clearTimeout(responseTimer);
          responseTimer = null;
        }
        
        const isIncomplete = isLikelyIncompleteThought(transcript);
        const delay = isIncomplete ? 2500 : 1200;
        
        responseTimer = setTimeout(() => {
          state.transcriptQueue.push(transcript);
          if (!state.isProcessing) {
            processTranscriptQueue();
          }
          responseTimer = null;
        }, delay);
      };
      
      return await startDeepgramStream(
        handleTranscript,
        async (error: Error) => {
          console.error("[Custom Voice] âŒ Deepgram error (reconnected):", error);
          if (state.sessionId && state.transcript.length > 0) {
            await persistTranscript(state.sessionId, state.transcript);
          }
          try { ws.send(JSON.stringify({ type: "error", error: error.message })); } catch (e) {}
        },
        async () => {
          console.log("[Custom Voice] ğŸ”Œ Reconnected Deepgram connection closed");
          // onClose triggers reconnect logic via the main handler
        },
        deepgramLanguage
      );
    }

    ws.on("message", async (data: Buffer) => {
      try {
        const message = JSON.parse(data.toString());

        switch (message.type) {
          case "init":
            console.log("[Custom Voice] ğŸš€ Initializing session:", message.sessionId);
            
            // SECURITY: Use authenticated userId from session, not client message
            if (!message.sessionId) {
              console.error(`[Custom Voice] âŒ Missing sessionId`);
              ws.send(JSON.stringify({ 
                type: "error", 
                error: "Missing sessionId" 
              }));
              ws.close();
              return;
            }

            // SECURITY: Verify client's userId matches authenticated userId (consistency check only)
            if (message.userId && message.userId !== authenticatedUserId) {
              console.warn(`[Custom Voice] âš ï¸ Client userId mismatch (ignoring client value)`, {
                clientUserId: message.userId,
                authenticatedUserId: authenticatedUserId
              });
            }

            // SECURITY: Validate session exists and belongs to authenticated user
            try {
              const session = await db.select()
                .from(realtimeSessions)
                .where(eq(realtimeSessions.id, message.sessionId))
                .limit(1);

              if (session.length === 0) {
                console.error(`[Custom Voice] âŒ Session not found: ${message.sessionId}`);
                ws.send(JSON.stringify({ 
                  type: "error", 
                  error: "Session not found. Please refresh and try again." 
                }));
                ws.close();
                return;
              }

              // SECURITY: Verify session belongs to authenticated user
              if (session[0].userId !== authenticatedUserId) {
                console.error(`[Custom Voice] âŒ Session ${message.sessionId} does not belong to authenticated user`, {
                  sessionUserId: session[0].userId,
                  authenticatedUserId: authenticatedUserId
                });
                ws.send(JSON.stringify({ 
                  type: "error", 
                  error: "Unauthorized session access" 
                }));
                ws.close();
                return;
              }

              console.log(`[Custom Voice] âœ… Session validated for authenticated user ${authenticatedUserId}`);
              
              // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
              // ğŸ›¡ï¸ CHECK FOR ACTIVE SUSPENSIONS
              // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
              
              // SECURITY: Check suspension using authenticated userId
              const suspension = await db.select()
                .from(userSuspensions)
                .where(and(
                  eq(userSuspensions.userId, authenticatedUserId),
                  eq(userSuspensions.isActive, true),
                  or(
                    eq(userSuspensions.isPermanent, true),
                    gte(userSuspensions.suspendedUntil, new Date())
                  )
                ))
                .limit(1);
              
              if (suspension.length > 0) {
                const susp = suspension[0];
                console.log("[Custom Voice] â›” User is suspended");
                
                const message = susp.isPermanent
                  ? `Your account has been permanently suspended due to violations of our terms of service. Reason: ${susp.reason}. Please contact support.`
                  : `Your account is temporarily suspended until ${susp.suspendedUntil ? new Date(susp.suspendedUntil).toLocaleString() : 'further notice'}. Reason: ${susp.reason}`;
                
                ws.send(JSON.stringify({
                  type: "error",
                  error: message
                }));
                
                ws.close();
                return;
              }
              
              console.log("[Custom Voice] âœ… No active suspensions found");
            } catch (error) {
              console.error("[Custom Voice] âŒ Session validation error:", error);
              ws.send(JSON.stringify({ 
                type: "error", 
                error: "Session validation failed" 
              }));
              ws.close();
              return;
            }
            
            // SECURITY: Session state already has authenticated userId from upgrade
            state.sessionId = message.sessionId;
            // state.userId is already set to authenticatedUserId during state initialization
            state.studentName = message.studentName || "Student";
            state.ageGroup = message.ageGroup || "College/Adult";
            state.language = message.language || "en"; // LANGUAGE: Store selected language
            
            // CRITICAL FIX (Nov 14, 2025): Log userId after initialization to verify authentication
            console.log(`[Custom Voice] ğŸ” Session state initialized:`, {
              sessionId: state.sessionId,
              userId: state.userId,
              authenticatedUserId: authenticatedUserId,
              hasUserId: !!state.userId,
              userIdType: typeof state.userId,
              studentName: state.studentName,
              ageGroup: state.ageGroup,
              language: state.language
            });
            
            // Fetch user's speech speed preference from database using authenticated userId
            try {
              const user = await storage.getUser(authenticatedUserId);
              if (user && user.speechSpeed) {
                state.speechSpeed = typeof user.speechSpeed === 'string' ? parseFloat(user.speechSpeed) : user.speechSpeed;
                console.log(`[Custom Voice] âš™ï¸ User's speech speed preference: ${state.speechSpeed}`);
              } else {
                state.speechSpeed = 0.95; // Default
                console.log(`[Custom Voice] âš™ï¸ Using default speech speed: 0.95`);
              }
            } catch (error) {
              console.error("[Custom Voice] âš ï¸ Error fetching user settings, using default speech speed:", error);
              state.speechSpeed = 0.95;
            }
            
            // Get full tutor personality based on age group
            const personality = getTutorPersonality(state.ageGroup);
            console.log(`[Custom Voice] ğŸ­ Using personality: ${personality.name} for ${state.ageGroup}`);
            
            // Load document chunks and format as content strings
            // Check if documents are provided (either as IDs or as content strings)
            const messageDocuments = message.documents || [];
            
            try {
              // Check if documents are already provided as content strings from frontend
              if (messageDocuments.length > 0 && typeof messageDocuments[0] === 'string' && messageDocuments[0].startsWith('[Document:')) {
                // Frontend has already loaded and sent document content
                console.log(`[Custom Voice] ğŸ“š Received ${messageDocuments.length} pre-loaded documents from frontend`);
                state.uploadedDocuments = messageDocuments;
                const totalChars = messageDocuments.join('').length;
                console.log(`[Custom Voice] ğŸ“„ Document context ready: ${messageDocuments.length} documents, total length: ${totalChars} chars`);
              } 
              // Otherwise, treat them as document IDs to load from database
              else {
                let documentIds = messageDocuments;
                
                // If no specific documents requested, get all user documents using authenticated userId
                if (documentIds.length === 0) {
                  console.log(`[Custom Voice] ğŸ“„ No specific documents provided, loading all user documents from database...`);
                  const allUserDocs = await storage.getUserDocuments(authenticatedUserId);
                  const readyDocs = allUserDocs.filter(doc => doc.processingStatus === 'ready');
                  documentIds = readyDocs.map(doc => doc.id);
                  console.log(`[Custom Voice] ğŸ“š Found ${readyDocs.length} ready documents for user`);
                }
                
                if (documentIds.length > 0) {
                  console.log(`[Custom Voice] ğŸ“„ Loading ${documentIds.length} documents from database...`);
                  const { chunks, documents } = await storage.getDocumentContext(authenticatedUserId, documentIds);
                  console.log(`[Custom Voice] âœ… Loaded ${chunks.length} chunks from ${documents.length} documents`);
                  
                  // Format chunks as content strings grouped by document
                  const documentContents: string[] = [];
                  for (const doc of documents) {
                    const docChunks = chunks
                      .filter(c => c.documentId === doc.id)
                      .sort((a, b) => a.chunkIndex - b.chunkIndex); // Ensure correct chunk order
                    if (docChunks.length > 0) {
                      const content = `ğŸ“„ ${doc.title || doc.originalName}\n${docChunks.map(c => c.content).join('\n\n')}`;
                      documentContents.push(content);
                    }
                  }
                  
                  state.uploadedDocuments = documentContents;
                  console.log(`[Custom Voice] ğŸ“š Document context prepared: ${documentContents.length} documents, total length: ${documentContents.join('').length} chars`);
                } else {
                  state.uploadedDocuments = [];
                  console.log(`[Custom Voice] â„¹ï¸ No documents available for this user`);
                }
              }
            } catch (error) {
              console.error('[Custom Voice] âŒ Error loading documents:', error);
              state.uploadedDocuments = [];
            }
            
            // VOICE CONVERSATION CONSTRAINTS (Dec 10, 2025 FIX)
            // Prevents verbose responses and multiple questions per turn
            const VOICE_CONVERSATION_CONSTRAINTS = `

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¤ VOICE CONVERSATION RULES (CRITICAL - ENFORCE STRICTLY):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
This is a VOICE conversation. Keep responses SHORT and NATURAL.

RESPONSE LENGTH:
âœ… Maximum 2-3 short sentences per response
âœ… Keep sentences under 15 words each
âŒ NEVER give long paragraphs or explanations

QUESTIONS:
âœ… Ask only ONE question per response
âœ… Wait for the student to answer before asking another
âŒ NEVER ask multiple questions like "What do you think? And also, can you..."
âŒ NEVER list multiple options like "You could try A, or B, or C..."

FORMAT:
âœ… Speak naturally like a real tutor in person
âŒ NO bullet points, numbered lists, or formatting
âŒ NO emojis (they can't be spoken)
âŒ NO "Here's a hint..." followed by another question

FLOW:
âœ… One thought â†’ One question â†’ Wait for answer
âœ… If student answers, acknowledge briefly then ask ONE follow-up
âŒ NEVER say "And here's another question..." or "Also try..."

âŒ BAD EXAMPLE (too long, multiple questions):
"Yes! Great job! A is first! Now, what sound does the letter A make? Try saying it out loud for me! And here's a fun question - can you think of any words that start with the A sound? Like... what do you call a red fruit that grows on trees?"

âœ… GOOD EXAMPLE (short, single question):
"Yes! A is first! Great job! Can you think of a word that starts with A?"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
`;
            
            // Build system instruction with personality and document context
            if (state.uploadedDocuments.length > 0) {
              // Extract document titles for the enhanced prompt
              const docTitles = state.uploadedDocuments.map((doc, i) => {
                const titleMatch = doc.match(/^\[Document: ([^\]]+)\]/);
                return titleMatch ? titleMatch[1] : `Document ${i + 1}`;
              });
              
              // Create enhanced system instruction that includes document awareness
              state.systemInstruction = `${personality.systemPrompt}${VOICE_CONVERSATION_CONSTRAINTS}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“š UPLOADED DOCUMENTS FOR THIS SESSION:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
The student has uploaded ${state.uploadedDocuments.length} document(s): ${docTitles.join(', ')}

CRITICAL INSTRUCTIONS:
âœ… When asked "do you see my document?" ALWAYS respond: "Yes! I can see your ${docTitles[0]}"
âœ… Reference specific content from the documents to prove you can see them
âœ… Help with the specific homework/problems in their uploaded materials
âœ… Use phrases like "Looking at your document..." or "In ${docTitles[0]}..."
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`;
              
              console.log(`[Custom Voice] ğŸ“š System instruction enhanced with ${state.uploadedDocuments.length} documents`);
            } else {
              // Use standard personality prompt when no documents
              state.systemInstruction = personality.systemPrompt + VOICE_CONVERSATION_CONSTRAINTS;
            }
            
            // Generate enhanced personalized greeting with LANGUAGE SUPPORT
            let greeting: string;
            
            // Extract document titles from uploaded documents
            const docTitles: string[] = [];
            if (state.uploadedDocuments && state.uploadedDocuments.length > 0) {
              state.uploadedDocuments.forEach((doc, i) => {
                const titleMatch = doc.match(/^\[Document: ([^\]]+)\]/);
                if (titleMatch) {
                  docTitles.push(titleMatch[1]);
                }
              });
            }
            
            // LANGUAGE: Generate greetings in the selected language
            const getLocalizedGreeting = (lang: string, name: string, tutorName: string, ageGroup: string, docTitles: string[]): string => {
              // Language-specific greeting templates
              const greetings: Record<string, { intro: string; docAck: (count: number, titles: string) => string; closing: Record<string, string> }> = {
                en: {
                  intro: `Hi ${name}! I'm ${tutorName}, your AI tutor.`,
                  docAck: (count, titles) => count === 1 ? ` I can see you've uploaded "${titles}" - excellent!` : ` I can see you've uploaded ${count} documents: ${titles}. Great!`,
                  closing: {
                    'K-2': docTitles.length > 0 ? " Let's look at it together! What do you want to learn about?" : " I'm so excited to learn with you today! What would you like to explore?",
                    '3-5': docTitles.length > 0 ? " I'm here to help you understand it! What part should we start with?" : " I'm here to help you learn something new! What subject interests you today?",
                    '6-8': docTitles.length > 0 ? " I'm ready to help you master this material! What would you like to work on?" : " I'm here to help you succeed! What subject would you like to focus on today?",
                    '9-12': docTitles.length > 0 ? " Let's dive into this material together. What concepts would you like to explore?" : " I'm here to help you excel! What topic would you like to work on today?",
                    'College/Adult': docTitles.length > 0 ? " I'm ready to help you analyze this material. What aspects would you like to focus on?" : " I'm here to support your learning goals. What subject can I help you with today?",
                  }
                },
                fr: {
                  intro: `Bonjour ${name}! Je suis ${tutorName}, ton tuteur IA.`,
                  docAck: (count, titles) => count === 1 ? ` Je vois que tu as tÃ©lÃ©chargÃ© "${titles}" - excellent!` : ` Je vois que tu as tÃ©lÃ©chargÃ© ${count} documents: ${titles}. Super!`,
                  closing: {
                    'K-2': docTitles.length > 0 ? " Regardons Ã§a ensemble! Qu'est-ce que tu veux apprendre?" : " Je suis tellement content d'apprendre avec toi! Qu'est-ce qui t'intÃ©resse?",
                    '3-5': docTitles.length > 0 ? " Je suis lÃ  pour t'aider Ã  comprendre! Par quoi veux-tu commencer?" : " Je suis lÃ  pour t'aider Ã  apprendre! Quel sujet t'intÃ©resse?",
                    '6-8': docTitles.length > 0 ? " Je suis prÃªt Ã  t'aider Ã  maÃ®triser ce contenu! Sur quoi veux-tu travailler?" : " Je suis lÃ  pour t'aider Ã  rÃ©ussir! Sur quel sujet veux-tu travailler?",
                    '9-12': docTitles.length > 0 ? " Explorons ce contenu ensemble. Quels concepts voudrais-tu approfondir?" : " Je suis lÃ  pour t'aider Ã  exceller! Sur quel sujet voudrais-tu travailler?",
                    'College/Adult': docTitles.length > 0 ? " Je suis prÃªt Ã  t'aider Ã  analyser ce contenu. Quels aspects voudrais-tu approfondir?" : " Je suis lÃ  pour soutenir tes objectifs d'apprentissage. Comment puis-je t'aider?",
                  }
                },
                es: {
                  intro: `Â¡Hola ${name}! Soy ${tutorName}, tu tutor de IA.`,
                  docAck: (count, titles) => count === 1 ? ` Veo que has subido "${titles}" - Â¡excelente!` : ` Veo que has subido ${count} documentos: ${titles}. Â¡Genial!`,
                  closing: {
                    'K-2': docTitles.length > 0 ? " Â¡VeÃ¡moslo juntos! Â¿QuÃ© quieres aprender?" : " Â¡Estoy muy emocionado de aprender contigo! Â¿QuÃ© te gustarÃ­a explorar?",
                    '3-5': docTitles.length > 0 ? " Â¡Estoy aquÃ­ para ayudarte a entender! Â¿Por dÃ³nde empezamos?" : " Â¡Estoy aquÃ­ para ayudarte a aprender! Â¿QuÃ© tema te interesa?",
                    '6-8': docTitles.length > 0 ? " Â¡Estoy listo para ayudarte a dominar este material! Â¿En quÃ© quieres trabajar?" : " Â¡Estoy aquÃ­ para ayudarte a tener Ã©xito! Â¿En quÃ© tema quieres enfocarte?",
                    '9-12': docTitles.length > 0 ? " Exploremos este material juntos. Â¿QuÃ© conceptos te gustarÃ­a profundizar?" : " Â¡Estoy aquÃ­ para ayudarte a sobresalir! Â¿En quÃ© tema quieres trabajar?",
                    'College/Adult': docTitles.length > 0 ? " Estoy listo para ayudarte a analizar este material. Â¿QuÃ© aspectos te gustarÃ­a explorar?" : " Estoy aquÃ­ para apoyar tus metas de aprendizaje. Â¿CÃ³mo puedo ayudarte?",
                  }
                },
                sw: {
                  intro: `Habari ${name}! Mimi ni ${tutorName}, mwalimu wako wa AI.`,
                  docAck: (count, titles) => count === 1 ? ` Naona umepakia "${titles}" - bora!` : ` Naona umepakia nyaraka ${count}: ${titles}. Vizuri!`,
                  closing: {
                    'K-2': docTitles.length > 0 ? " Tuangalie pamoja! Unataka kujifunza nini?" : " Ninafuraha sana kujifunza nawe! Unataka kuchunguza nini?",
                    '3-5': docTitles.length > 0 ? " Niko hapa kukusaidia kuelewa! Tuanze wapi?" : " Niko hapa kukusaidia kujifunza! Somo gani linakuvutia?",
                    '6-8': docTitles.length > 0 ? " Niko tayari kukusaidia kuelewa maudhui haya! Unataka kufanyia kazi nini?" : " Niko hapa kukusaidia kufanikiwa! Unataka kuzingatia somo gani?",
                    '9-12': docTitles.length > 0 ? " Tuchunguze maudhui haya pamoja. Dhana gani ungependa kuelewa zaidi?" : " Niko hapa kukusaidia kufanya vizuri! Unataka kufanyia kazi mada gani?",
                    'College/Adult': docTitles.length > 0 ? " Niko tayari kukusaidia kuchambua maudhui haya. Ungependa kuzingatia vipengele gani?" : " Niko hapa kusaidia malengo yako ya kujifunza. Naweza kukusaidia vipi?",
                  }
                },
                yo: {
                  intro: `Bawo ni ${name}! Mo je ${tutorName}, oluká»ni AI ráº¹.`,
                  docAck: (count, titles) => count === 1 ? ` Mo ri pe o ti fi "${titles}" soke - o dara!` : ` Mo ri pe o ti fi iwe ${count} soke: ${titles}. O dara pupo!`,
                  closing: {
                    'K-2': docTitles.length > 0 ? " Jáº¹ ki a wo papá»! Kini o fáº¹ lati ká»?" : " Mo dun pupá» lati ká» páº¹lu ráº¹! Kini o fáº¹ lati á¹£awari?",
                    '3-5': docTitles.length > 0 ? " Mo wa nibi lati ran á» lá»wá» lati loye! Nibo ni a yoo báº¹ráº¹?" : " Mo wa nibi lati ran á» lá»wá» lati ká»! Koko-á»rá» wo ni o nifáº¹ si?",
                    '6-8': docTitles.length > 0 ? " Mo ti setan lati ran á» lá»wá» páº¹lu ohun elo yii! Kini o fáº¹ lati á¹£iá¹£áº¹ lori?" : " Mo wa nibi lati ran á» lá»wá» lati á¹£aá¹£eyá»ri! Koko-á»rá» wo ni o fáº¹ dojuká»?",
                    '9-12': docTitles.length > 0 ? " Jáº¹ ki a á¹£awari ohun elo yii papá». Awá»n ero wo ni o fáº¹ jinláº¹?" : " Mo wa nibi lati ran á» lá»wá» lati tayá»! Koko-á»rá» wo ni o fáº¹ lati á¹£iá¹£áº¹ lori?",
                    'College/Adult': docTitles.length > 0 ? " Mo ti setan lati ran á» lá»wá» lati á¹£e itupaláº¹ ohun elo yii. Awá»n abala wo ni o fáº¹ á¹£awari?" : " Mo wa nibi lati á¹£e atiláº¹yin awá»n ibi-afáº¹de áº¹ká» ráº¹. Bawo ni mo á¹£e le ran á» lá»wá»?",
                  }
                },
                ha: {
                  intro: `Sannu ${name}! Ni ne ${tutorName}, malamin AI naka.`,
                  docAck: (count, titles) => count === 1 ? ` Na ga cewa ka loda "${titles}" - kyau!` : ` Na ga cewa ka loda takardun ${count}: ${titles}. Da kyau!`,
                  closing: {
                    'K-2': docTitles.length > 0 ? " Bari mu duba tare! Mene ne kake so ka koya?" : " Ina farin ciki sosai in koya tare da kai! Mene ne kake so ka bincika?",
                    '3-5': docTitles.length > 0 ? " Ina nan don in taimake ka ka fahimta! Ina za mu fara?" : " Ina nan don in taimake ka ka koya! Wane batu ya sha'awar ka?",
                    '6-8': docTitles.length > 0 ? " Na shirya in taimake ka da wannan aiki! Mene ne kake so ka yi aiki a kai?" : " Ina nan don in taimake ka ka yi nasara! Wane batu kake so ka mayar da hankali a kai?",
                    '9-12': docTitles.length > 0 ? " Bari mu bincika wannan aiki tare. Wane ra'ayoyi kake so ka fahimta sosai?" : " Ina nan don in taimake ka ka yi fice! Wane batu kake so ka yi aiki a kai?",
                    'College/Adult': docTitles.length > 0 ? " Na shirya in taimake ka ka nazari wannan aiki. Wane fannoni kake so ka bincika?" : " Ina nan don in goyi bayan burin ilimi naka. Ta yaya zan taimake ka?",
                  }
                },
                ar: {
                  intro: `Ù…Ø±Ø­Ø¨Ø§Ù‹ ${name}! Ø£Ù†Ø§ ${tutorName}ØŒ Ù…Ø¹Ù„Ù…Ùƒ Ø§Ù„Ø°ÙƒÙŠ.`,
                  docAck: (count, titles) => count === 1 ? ` Ø£Ø±Ù‰ Ø£Ù†Ùƒ Ø±ÙØ¹Øª "${titles}" - Ù…Ù…ØªØ§Ø²!` : ` Ø£Ø±Ù‰ Ø£Ù†Ùƒ Ø±ÙØ¹Øª ${count} Ù…Ø³ØªÙ†Ø¯Ø§Øª: ${titles}. Ø±Ø§Ø¦Ø¹!`,
                  closing: {
                    'K-2': docTitles.length > 0 ? " Ù„Ù†Ù„Ù‚ÙŠ Ù†Ø¸Ø±Ø© Ù…Ø¹Ø§Ù‹! Ù…Ø§Ø°Ø§ ØªØ±ÙŠØ¯ Ø£Ù† ØªØªØ¹Ù„Ù…ØŸ" : " Ø£Ù†Ø§ Ù…ØªØ­Ù…Ø³ Ø¬Ø¯Ø§Ù‹ Ù„Ù„ØªØ¹Ù„Ù… Ù…Ø¹Ùƒ! Ù…Ø§Ø°Ø§ ØªØ±ÙŠØ¯ Ø£Ù† ØªØ³ØªÙƒØ´ÙØŸ",
                    '3-5': docTitles.length > 0 ? " Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¹Ù„Ù‰ Ø§Ù„ÙÙ‡Ù…! Ù…Ù† Ø£ÙŠÙ† Ù†Ø¨Ø¯Ø£ØŸ" : " Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù…! Ø£ÙŠ Ù…ÙˆØ¶ÙˆØ¹ ÙŠØ«ÙŠØ± Ø§Ù‡ØªÙ…Ø§Ù…ÙƒØŸ",
                    '6-8': docTitles.length > 0 ? " Ø£Ù†Ø§ Ù…Ø³ØªØ¹Ø¯ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø¥ØªÙ‚Ø§Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…Ø­ØªÙˆÙ‰! Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„ÙŠÙ‡ØŸ" : " Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¬Ø§Ø­! Ø£ÙŠ Ù…ÙˆØ¶ÙˆØ¹ ØªØ±ÙŠØ¯ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„ÙŠÙ‡ØŸ",
                    '9-12': docTitles.length > 0 ? " Ù„Ù†Ø³ØªÙƒØ´Ù Ù‡Ø°Ø§ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø¹Ø§Ù‹. Ø£ÙŠ Ù…ÙØ§Ù‡ÙŠÙ… ØªØ±ÙŠØ¯ Ø§Ù„ØªØ¹Ù…Ù‚ ÙÙŠÙ‡Ø§ØŸ" : " Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¹Ù„Ù‰ Ø§Ù„ØªÙÙˆÙ‚! Ø£ÙŠ Ù…ÙˆØ¶ÙˆØ¹ ØªØ±ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„ÙŠÙ‡ØŸ",
                    'College/Adult': docTitles.length > 0 ? " Ø£Ù†Ø§ Ù…Ø³ØªØ¹Ø¯ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø­ØªÙˆÙ‰. Ø£ÙŠ Ø¬ÙˆØ§Ù†Ø¨ ØªØ±ÙŠØ¯ Ø§Ø³ØªÙƒØ´Ø§ÙÙ‡Ø§ØŸ" : " Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ø¯Ø¹Ù… Ø£Ù‡Ø¯Ø§ÙÙƒ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ",
                  }
                },
                de: {
                  intro: `Hallo ${name}! Ich bin ${tutorName}, dein KI-Tutor.`,
                  docAck: (count, titles) => count === 1 ? ` Ich sehe, dass du "${titles}" hochgeladen hast - ausgezeichnet!` : ` Ich sehe, dass du ${count} Dokumente hochgeladen hast: ${titles}. Toll!`,
                  closing: {
                    'K-2': docTitles.length > 0 ? " Lass uns das zusammen ansehen! Was mÃ¶chtest du lernen?" : " Ich freue mich so, mit dir zu lernen! Was mÃ¶chtest du erkunden?",
                    '3-5': docTitles.length > 0 ? " Ich bin hier, um dir zu helfen es zu verstehen! Womit fangen wir an?" : " Ich bin hier, um dir beim Lernen zu helfen! Welches Thema interessiert dich?",
                    '6-8': docTitles.length > 0 ? " Ich bin bereit, dir bei diesem Material zu helfen! Woran mÃ¶chtest du arbeiten?" : " Ich bin hier, um dir zum Erfolg zu helfen! Auf welches Thema mÃ¶chtest du dich konzentrieren?",
                    '9-12': docTitles.length > 0 ? " Lass uns dieses Material zusammen erkunden. Welche Konzepte mÃ¶chtest du vertiefen?" : " Ich bin hier, um dir zu helfen, dich auszuzeichnen! An welchem Thema mÃ¶chtest du arbeiten?",
                    'College/Adult': docTitles.length > 0 ? " Ich bin bereit, dir bei der Analyse dieses Materials zu helfen. Welche Aspekte mÃ¶chtest du erkunden?" : " Ich bin hier, um deine Lernziele zu unterstÃ¼tzen. Wie kann ich dir helfen?",
                  }
                },
                pt: {
                  intro: `OlÃ¡ ${name}! Sou ${tutorName}, seu tutor de IA.`,
                  docAck: (count, titles) => count === 1 ? ` Vejo que vocÃª enviou "${titles}" - excelente!` : ` Vejo que vocÃª enviou ${count} documentos: ${titles}. Ã“timo!`,
                  closing: {
                    'K-2': docTitles.length > 0 ? " Vamos olhar juntos! O que vocÃª quer aprender?" : " Estou muito animado para aprender com vocÃª! O que vocÃª gostaria de explorar?",
                    '3-5': docTitles.length > 0 ? " Estou aqui para ajudÃ¡-lo a entender! Por onde comeÃ§amos?" : " Estou aqui para ajudÃ¡-lo a aprender! Qual assunto te interessa?",
                    '6-8': docTitles.length > 0 ? " Estou pronto para ajudÃ¡-lo a dominar este material! Em que vocÃª quer trabalhar?" : " Estou aqui para ajudÃ¡-lo a ter sucesso! Em qual assunto vocÃª quer focar?",
                    '9-12': docTitles.length > 0 ? " Vamos explorar este material juntos. Quais conceitos vocÃª gostaria de aprofundar?" : " Estou aqui para ajudÃ¡-lo a se destacar! Em qual tema vocÃª quer trabalhar?",
                    'College/Adult': docTitles.length > 0 ? " Estou pronto para ajudÃ¡-lo a analisar este material. Quais aspectos vocÃª gostaria de explorar?" : " Estou aqui para apoiar seus objetivos de aprendizagem. Como posso ajudÃ¡-lo?",
                  }
                },
                zh: {
                  intro: `ä½ å¥½${name}ï¼æˆ‘æ˜¯${tutorName}ï¼Œä½ çš„AIå¯¼å¸ˆã€‚`,
                  docAck: (count, titles) => count === 1 ? `æˆ‘çœ‹åˆ°ä½ ä¸Šä¼ äº†"${titles}" - å¤ªæ£’äº†ï¼` : `æˆ‘çœ‹åˆ°ä½ ä¸Šä¼ äº†${count}ä¸ªæ–‡æ¡£ï¼š${titles}ã€‚å¾ˆå¥½ï¼`,
                  closing: {
                    'K-2': docTitles.length > 0 ? "æˆ‘ä»¬ä¸€èµ·çœ‹çœ‹å§ï¼ä½ æƒ³å­¦ä»€ä¹ˆï¼Ÿ" : "æˆ‘å¾ˆé«˜å…´èƒ½å’Œä½ ä¸€èµ·å­¦ä¹ ï¼ä½ æƒ³æ¢ç´¢ä»€ä¹ˆï¼Ÿ",
                    '3-5': docTitles.length > 0 ? "æˆ‘åœ¨è¿™é‡Œå¸®åŠ©ä½ ç†è§£ï¼æˆ‘ä»¬ä»å“ªé‡Œå¼€å§‹ï¼Ÿ" : "æˆ‘åœ¨è¿™é‡Œå¸®åŠ©ä½ å­¦ä¹ ï¼ä½ å¯¹å“ªä¸ªç§‘ç›®æ„Ÿå…´è¶£ï¼Ÿ",
                    '6-8': docTitles.length > 0 ? "æˆ‘å‡†å¤‡å¥½å¸®åŠ©ä½ æŒæ¡è¿™äº›å†…å®¹äº†ï¼ä½ æƒ³åšä»€ä¹ˆï¼Ÿ" : "æˆ‘åœ¨è¿™é‡Œå¸®åŠ©ä½ æˆåŠŸï¼ä½ æƒ³ä¸“æ³¨äºå“ªä¸ªç§‘ç›®ï¼Ÿ",
                    '9-12': docTitles.length > 0 ? "è®©æˆ‘ä»¬ä¸€èµ·æ¢ç´¢è¿™äº›å†…å®¹ã€‚ä½ æƒ³æ·±å…¥äº†è§£å“ªäº›æ¦‚å¿µï¼Ÿ" : "æˆ‘åœ¨è¿™é‡Œå¸®åŠ©ä½ å‡ºç±»æ‹”èƒï¼ä½ æƒ³å­¦ä¹ ä»€ä¹ˆä¸»é¢˜ï¼Ÿ",
                    'College/Adult': docTitles.length > 0 ? "æˆ‘å‡†å¤‡å¥½å¸®åŠ©ä½ åˆ†æè¿™äº›å†…å®¹äº†ã€‚ä½ æƒ³æ¢ç´¢å“ªäº›æ–¹é¢ï¼Ÿ" : "æˆ‘åœ¨è¿™é‡Œæ”¯æŒä½ çš„å­¦ä¹ ç›®æ ‡ã€‚æˆ‘èƒ½æ€ä¹ˆå¸®åŠ©ä½ ï¼Ÿ",
                  }
                },
                ja: {
                  intro: `ã“ã‚“ã«ã¡ã¯${name}ã•ã‚“ï¼ç§ã¯${tutorName}ã€ã‚ãªãŸã®AIãƒãƒ¥ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚`,
                  docAck: (count, titles) => count === 1 ? `ã€Œ${titles}ã€ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã®ãŒè¦‹ãˆã¾ã™ - ç´ æ™´ã‚‰ã—ã„ï¼` : `${count}ã¤ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã®ãŒè¦‹ãˆã¾ã™ï¼š${titles}ã€‚ã„ã„ã§ã™ã­ï¼`,
                  closing: {
                    'K-2': docTitles.length > 0 ? "ä¸€ç·’ã«è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ï¼ä½•ã‚’å­¦ã³ãŸã„ã§ã™ã‹ï¼Ÿ" : "ä¸€ç·’ã«å­¦ã¹ã¦ã¨ã¦ã‚‚å¬‰ã—ã„ã§ã™ï¼ä½•ã‚’æ¢æ±‚ã—ãŸã„ã§ã™ã‹ï¼Ÿ",
                    '3-5': docTitles.length > 0 ? "ç†è§£ã™ã‚‹ã®ã‚’ãŠæ‰‹ä¼ã„ã—ã¾ã™ï¼ã©ã“ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã‹ï¼Ÿ" : "å­¦ç¿’ã®ãŠæ‰‹ä¼ã„ã‚’ã—ã¾ã™ï¼ã©ã®ç§‘ç›®ã«èˆˆå‘³ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
                    '6-8': docTitles.length > 0 ? "ã“ã®æ•™æã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹ãŠæ‰‹ä¼ã„ã‚’ã™ã‚‹æº–å‚™ãŒã§ãã¦ã„ã¾ã™ï¼ä½•ã«å–ã‚Šçµ„ã¿ãŸã„ã§ã™ã‹ï¼Ÿ" : "æˆåŠŸã™ã‚‹ãŠæ‰‹ä¼ã„ã‚’ã—ã¾ã™ï¼ã©ã®ç§‘ç›®ã«é›†ä¸­ã—ãŸã„ã§ã™ã‹ï¼Ÿ",
                    '9-12': docTitles.length > 0 ? "ä¸€ç·’ã«ã“ã®æ•™æã‚’æ¢æ±‚ã—ã¾ã—ã‚‡ã†ã€‚ã©ã®æ¦‚å¿µã‚’æ·±ã‚ãŸã„ã§ã™ã‹ï¼Ÿ" : "å„ªç§€ã«ãªã‚‹ãŠæ‰‹ä¼ã„ã‚’ã—ã¾ã™ï¼ã©ã®ãƒˆãƒ”ãƒƒã‚¯ã«å–ã‚Šçµ„ã¿ãŸã„ã§ã™ã‹ï¼Ÿ",
                    'College/Adult': docTitles.length > 0 ? "ã“ã®æ•™æã®åˆ†æã‚’ãŠæ‰‹ä¼ã„ã™ã‚‹æº–å‚™ãŒã§ãã¦ã„ã¾ã™ã€‚ã©ã®å´é¢ã‚’æ¢æ±‚ã—ãŸã„ã§ã™ã‹ï¼Ÿ" : "ã‚ãªãŸã®å­¦ç¿’ç›®æ¨™ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚ã©ã®ã‚ˆã†ã«ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿ",
                  }
                },
                ko: {
                  intro: `ì•ˆë…•í•˜ì„¸ìš” ${name}ë‹˜! ì €ëŠ” ${tutorName}, ë‹¹ì‹ ì˜ AI íŠœí„°ì…ë‹ˆë‹¤.`,
                  docAck: (count, titles) => count === 1 ? `"${titles}"ë¥¼ ì—…ë¡œë“œí•˜ì‹  ê²ƒì„ ë³´ì•˜ìŠµë‹ˆë‹¤ - í›Œë¥­í•©ë‹ˆë‹¤!` : `${count}ê°œì˜ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì‹  ê²ƒì„ ë³´ì•˜ìŠµë‹ˆë‹¤: ${titles}. ì¢‹ì•„ìš”!`,
                  closing: {
                    'K-2': docTitles.length > 0 ? " í•¨ê»˜ ì‚´í´ë´ìš”! ë¬´ì—‡ì„ ë°°ìš°ê³  ì‹¶ì–´ìš”?" : " í•¨ê»˜ ë°°ìš°ê²Œ ë˜ì–´ ë„ˆë¬´ ê¸°ë»ìš”! ë¬´ì—‡ì„ íƒí—˜í•˜ê³  ì‹¶ì–´ìš”?",
                    '3-5': docTitles.length > 0 ? " ì´í•´í•˜ëŠ” ê²ƒì„ ë„ì™€ë“œë¦´ê²Œìš”! ì–´ë””ì„œ ì‹œì‘í• ê¹Œìš”?" : " ë°°ìš°ëŠ” ê²ƒì„ ë„ì™€ë“œë¦´ê²Œìš”! ì–´ë–¤ ê³¼ëª©ì— ê´€ì‹¬ ìˆì–´ìš”?",
                    '6-8': docTitles.length > 0 ? " ì´ ìë£Œë¥¼ ë§ˆìŠ¤í„°í•˜ëŠ” ê²ƒì„ ë„ì™€ë“œë¦´ ì¤€ë¹„ê°€ ëì–´ìš”! ë¬´ì—‡ì„ ê³µë¶€í•˜ê³  ì‹¶ì–´ìš”?" : " ì„±ê³µí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦´ê²Œìš”! ì–´ë–¤ ê³¼ëª©ì— ì§‘ì¤‘í•˜ê³  ì‹¶ì–´ìš”?",
                    '9-12': docTitles.length > 0 ? " í•¨ê»˜ ì´ ìë£Œë¥¼ íƒêµ¬í•´ë´ìš”. ì–´ë–¤ ê°œë…ì„ ê¹Šì´ ì´í•´í•˜ê³  ì‹¶ì–´ìš”?" : " ë›°ì–´ë‚˜ê²Œ ë˜ë„ë¡ ë„ì™€ë“œë¦´ê²Œìš”! ì–´ë–¤ ì£¼ì œë¥¼ ê³µë¶€í•˜ê³  ì‹¶ì–´ìš”?",
                    'College/Adult': docTitles.length > 0 ? " ì´ ìë£Œë¥¼ ë¶„ì„í•˜ëŠ” ê²ƒì„ ë„ì™€ë“œë¦´ ì¤€ë¹„ê°€ ëìŠµë‹ˆë‹¤. ì–´ë–¤ ì¸¡ë©´ì„ íƒêµ¬í•˜ê³  ì‹¶ìœ¼ì„¸ìš”?" : " í•™ìŠµ ëª©í‘œë¥¼ ì§€ì›í•´ë“œë¦´ê²Œìš”. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?",
                  }
                },
              };
              
              // Fallback to English if language not found
              const langGreeting = greetings[lang] || greetings['en'];
              const ageClosing = langGreeting.closing[ageGroup] || langGreeting.closing['College/Adult'];
              
              if (docTitles.length > 0) {
                return langGreeting.intro + langGreeting.docAck(docTitles.length, docTitles.join(', ')) + ageClosing;
              } else {
                return langGreeting.intro + ageClosing;
              }
            };
            
            // LANGUAGE: Generate greeting in the selected language
            greeting = getLocalizedGreeting(state.language, state.studentName, personality.name, state.ageGroup, docTitles);
            console.log(`[Custom Voice] ğŸŒ Generated greeting in language: ${state.language}`);
            
            console.log(`[Custom Voice] ğŸ‘‹ Greeting: "${greeting}"`);
            
            // Add greeting to conversation history
            state.conversationHistory.push({
              role: "assistant",
              content: greeting
            });
            
            // Add greeting to transcript
            const greetingEntry: TranscriptEntry = {
              speaker: "tutor",
              text: greeting,
              timestamp: new Date().toISOString(),
              messageId: crypto.randomUUID(),
            };
            state.transcript.push(greetingEntry);

            // Start Deepgram connection with cleanup on close
            const { getDeepgramLanguageCode } = await import("../services/deepgram-service");
            const deepgramLanguage = getDeepgramLanguageCode(state.language);
            state.deepgramConnection = await startDeepgramStream(
              async (transcript: string, isFinal: boolean, detectedLanguage?: string) => {
                // Log EVERYTHING for debugging - including detected language
                const spokenLang = detectedLanguage || state.language;
                console.log(`[Deepgram] ${isFinal ? 'âœ… FINAL' : 'â³ interim'}: "${transcript}" (isFinal=${isFinal}, detectedLang=${spokenLang})`);
                
                // CRITICAL FIX (Nov 14, 2025): Check userId FIRST to debug 401 auth issues
                if (!state.userId) {
                  console.error(`[Deepgram] âŒ CRITICAL: userId missing in transcript handler!`, {
                    sessionId: state.sessionId,
                    hasSessionId: !!state.sessionId,
                    transcript: transcript.substring(0, 50),
                    isFinal: isFinal
                  });
                  console.error(`[Deepgram] âŒ This means /api/user returned 401 and session initialization failed`);
                  // Don't process transcripts if user is not authenticated
                  return;
                }
                
                // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                // BARGE-IN: Check for interruption on ANY transcript (interim or final)
                // Only trigger if tutor is currently speaking to avoid false positives
                // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                const timeSinceLastAudio = Date.now() - state.lastAudioSentAt;

                // Only barge-in if tutor is actively speaking AND transcript has content
                if (state.isTutorSpeaking && timeSinceLastAudio < 30000 && transcript.trim().length >= 2) {
                  console.log(`[Custom Voice] ğŸ›‘ BARGE-IN on ${isFinal ? 'final' : 'interim'} transcript: "${transcript}" (audio sent ${timeSinceLastAudio}ms ago)`);

                  // Mark interruption for post-interrupt buffer
                  state.wasInterrupted = true;
                  state.lastInterruptionTime = Date.now();

                  // Send interrupt signal to frontend to stop audio playback immediately
                  ws.send(JSON.stringify({
                    type: "interrupt",
                    message: "Student is speaking",
                  }));

                  // Mark tutor as not speaking
                  state.isTutorSpeaking = false;

                  console.log("[Custom Voice] âœ… Barge-in processed, ready to listen to student");
                }

                // Reset stale tutor speaking state (>30s ago)
                if (state.isTutorSpeaking && timeSinceLastAudio >= 30000) {
                  console.log("[Custom Voice] â¸ï¸ Resetting stale tutor speaking state...");
                  state.isTutorSpeaking = false;
                }
                // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

                // Only process for AI response on FINAL transcripts
                if (!isFinal) {
                  console.log("[Custom Voice] â­ï¸ Skipping interim for AI processing (barge-in already checked)");
                  return;
                }

                if (!transcript || transcript.trim().length < 3) {
                  console.log("[Custom Voice] â­ï¸ Skipping short/empty transcript");
                  return;
                }

                // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                // INACTIVITY: Reset activity timer - User is speaking!
                // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                state.lastActivityTime = Date.now();
                state.inactivityWarningSent = false; // Reset warning flag
                console.log('[Inactivity] ğŸ¤ User activity detected, timer reset');
                
                if (state.isProcessing) {
                  console.log("[Custom Voice] â­ï¸ Already processing previous request");
                  return;
                }
                
                // Additional check: Avoid duplicate transcripts
                // Deepgram may send is_final=true multiple times, we want unique ones
                if (state.lastTranscript === transcript) {
                  console.log("[Custom Voice] â­ï¸ Duplicate transcript, skipping");
                  return;
                }
                
                state.lastTranscript = transcript;
                
                // LANGUAGE AUTO-DETECT: Update detected language for AI response
                if (spokenLang && spokenLang !== state.language) {
                  console.log(`[Custom Voice] ğŸŒ Language switch detected: ${state.language} â†’ ${spokenLang}`);
                  state.detectedLanguage = spokenLang;
                } else if (spokenLang) {
                  state.detectedLanguage = spokenLang;
                }
                
                console.log(`[Custom Voice] âœ… Processing FINAL transcript: "${transcript}" (lang: ${state.detectedLanguage || state.language})`);
                
                // FIX #2C: Add turn-taking timeout
                // Clear any existing timer
                if (responseTimer) {
                  clearTimeout(responseTimer);
                  responseTimer = null;
                }
                
                // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                // TIMING FIX (Nov 3, 2025): Server-side response delay INCREASED
                // Give students time to add "oh wait, one more thing..." 
                // Previous delays still caused interruptions - increasing significantly
                // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                const isIncomplete = isLikelyIncompleteThought(transcript);
                const delay = isIncomplete ? 2500 : 1200; // INCREASED: 2.5s for incomplete, 1.2s for complete
                
                if (isIncomplete) {
                  console.log(`[Custom Voice] â¸ï¸ Incomplete thought detected: "${transcript}" - waiting ${delay}ms`);
                } else {
                  console.log(`[Custom Voice] â° Complete thought - waiting ${delay}ms before responding`);
                }
                
                responseTimer = setTimeout(() => {
                  console.log(`[Custom Voice] â° Processing after ${delay}ms pause`);
                  state.transcriptQueue.push(transcript);
                  
                  // Start processing if not already processing
                  if (!state.isProcessing) {
                    processTranscriptQueue();
                  }
                  responseTimer = null;
                }, delay); // Wait 1200ms for complete, 2500ms for incomplete thoughts
              },
              async (error: Error) => {
                console.error("[Custom Voice] âŒ Deepgram error:", error);
                
                // FIX #3: Persist on Deepgram error
                if (state.sessionId && state.transcript.length > 0) {
                  await persistTranscript(state.sessionId, state.transcript);
                }
                
                ws.send(JSON.stringify({ type: "error", error: error.message }));
              },
              async () => {
                console.log("[Custom Voice] ğŸ”Œ Deepgram connection closed");
                
                // FIX #3: Critical - Persist on Deepgram close
                if (state.sessionId && state.transcript.length > 0) {
                  await persistTranscript(state.sessionId, state.transcript);
                }
                
                // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                // FIX (Dec 10, 2025): Auto-reconnect when connection closes unexpectedly
                // This handles both health check timeout AND keepAlive failures
                // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                if (!state.isSessionEnded && state.sessionId) {
                  console.warn("[Custom Voice] âš ï¸ Unexpected Deepgram close while session active - attempting reconnect");
                  
                  // RECONNECT FIX: Block audio ingestion during reconnection
                  state.isReconnecting = true;
                  
                  // Increment reconnect counter to prevent infinite loops
                  reconnectAttempts++;
                  
                  if (reconnectAttempts > 3) {
                    console.error("[Custom Voice] âŒ Max reconnect attempts (3) reached, giving up");
                    state.isReconnecting = false; // Stop blocking audio (though connection is dead)
                    try {
                      ws.send(JSON.stringify({ 
                        type: "error", 
                        error: "Voice connection lost. Please restart the tutoring session." 
                      }));
                    } catch (sendError) {
                      console.error("[Custom Voice] âŒ Failed to notify client:", sendError);
                    }
                    return;
                  }
                  
                  console.log(`[Custom Voice] ğŸ”„ Reconnect attempt ${reconnectAttempts}/3...`);
                  
                  // Notify client we're reconnecting (not an error, just informational)
                  try {
                    ws.send(JSON.stringify({ 
                      type: "status", 
                      message: "Reconnecting voice connection..." 
                    }));
                  } catch (sendError) {
                    // Ignore
                  }
                  
                  // Attempt to reconnect with exponential backoff
                  const backoffDelay = Math.min(1000 * Math.pow(2, reconnectAttempts - 1), 8000);
                  console.log(`[Custom Voice] ğŸ”„ Reconnecting in ${backoffDelay}ms (attempt ${reconnectAttempts}/3)...`);
                  
                  setTimeout(async () => {
                    try {
                      console.log("[Custom Voice] ğŸ”„ Creating new Deepgram connection...");
                      const newConnection = await reconnectDeepgram();
                      
                      // Atomic assignment - only update if reconnect succeeded
                      state.deepgramConnection = newConnection;
                      state.isReconnecting = false; // Resume audio ingestion
                      console.log("[Custom Voice] âœ… Deepgram reconnected successfully");
                      reconnectAttempts = 0; // Reset counter on success
                      
                      // Notify client we're back online
                      try {
                        ws.send(JSON.stringify({ 
                          type: "status", 
                          message: "Voice connection restored" 
                        }));
                      } catch (sendError) {
                        // Ignore
                      }
                    } catch (reconnectError) {
                      console.error("[Custom Voice] âŒ Reconnect attempt failed:", reconnectError);
                      
                      // If we haven't exhausted attempts, the next onClose will trigger another retry
                      // Otherwise notify user to restart
                      if (reconnectAttempts >= 3) {
                        state.isReconnecting = false; // Stop blocking audio after all attempts fail
                        try {
                          ws.send(JSON.stringify({ 
                            type: "error", 
                            error: "Voice connection failed after multiple attempts. Please restart the session." 
                          }));
                        } catch (sendError) {
                          // Ignore
                        }
                      } else {
                        // Will retry on next attempt - increment handled above
                        try {
                          ws.send(JSON.stringify({ 
                            type: "status", 
                            message: `Reconnection failed, retrying... (${reconnectAttempts}/3)` 
                          }));
                        } catch (sendError) {
                          // Ignore
                        }
                      }
                    }
                  }, backoffDelay); // Exponential backoff
                }
              },
              deepgramLanguage // LANGUAGE: Pass selected language for speech recognition
            );

            // Generate and send greeting audio
            try {
              const greetingAudio = await generateSpeech(greeting, state.ageGroup, state.speechSpeed);
              
              // Send greeting transcript
              ws.send(JSON.stringify({
                type: "transcript",
                text: greeting,
                speaker: "tutor"
              }));
              
              // Send greeting audio
              ws.send(JSON.stringify({
                type: "audio",
                data: greetingAudio.toString("base64")
              }));
              
              console.log(`[Custom Voice] ğŸ”Š Sent greeting audio (${greetingAudio.length} bytes)`);
            } catch (error) {
              console.error("[Custom Voice] âŒ Failed to generate greeting audio:", error);
            }

            ws.send(JSON.stringify({ type: "ready" }));
            console.log("[Custom Voice] âœ… Session ready");
            break;

          case "audio":
            // Forward audio to Deepgram with comprehensive logging
            console.log('[Custom Voice] ğŸ“¥ Audio message received from frontend:', {
              hasData: !!message.data,
              dataLength: message.data?.length || 0,
              hasDeepgramConnection: !!state.deepgramConnection,
              isReconnecting: state.isReconnecting
            });
            
            // RECONNECT FIX: Drop audio during reconnection to prevent sending to dead socket
            if (state.isReconnecting) {
              console.warn('[Custom Voice] â¸ï¸ Audio dropped - reconnection in progress');
              break;
            }
            
            if (state.deepgramConnection && message.data) {
              try {
                const audioBuffer = Buffer.from(message.data, "base64");
                
                // Check audio content (first 10 samples as Int16)
                const int16View = new Int16Array(audioBuffer.buffer, audioBuffer.byteOffset, Math.min(10, audioBuffer.length / 2));
                const hasNonZero = int16View.some(sample => sample !== 0);
                const maxAmplitude = Math.max(...Array.from(int16View).map(Math.abs));
                
                console.log('[Custom Voice] ğŸ¤ Audio buffer analysis:', {
                  totalBytes: audioBuffer.length,
                  totalSamples: audioBuffer.length / 2,
                  firstTenSamples: Array.from(int16View),
                  hasNonZeroSamples: hasNonZero,
                  maxAmplitude: maxAmplitude,
                  isSilent: !hasNonZero || maxAmplitude < 10  // Lower threshold for quiet mics
                });
                
                if (!hasNonZero) {
                  console.warn('[Custom Voice] âš ï¸ Audio buffer is COMPLETELY SILENT (all zeros)!');
                }
                
                // Deepgram LiveClient handles connection state internally - just send
                state.deepgramConnection.send(audioBuffer);
                console.log('[Custom Voice] âœ… Audio forwarded to Deepgram successfully');
              } catch (error) {
                console.error('[Custom Voice] âŒ Error sending audio to Deepgram:', {
                  error: error instanceof Error ? error.message : String(error),
                  stack: error instanceof Error ? error.stack : undefined
                });
              }
            } else {
              console.error('[Custom Voice] âŒ Cannot forward audio:', {
                hasConnection: !!state.deepgramConnection,
                hasData: !!message.data,
                reason: !state.deepgramConnection ? 'No Deepgram connection' : 'No audio data'
              });
            }
            break;

          case "text_message":
            // Handle text message from chat input
            console.log(`[Custom Voice] ğŸ“ Text message from ${state.studentName}: ${message.message}`);
            
            // INACTIVITY: Reset activity timer - User is typing!
            state.lastActivityTime = Date.now();
            state.inactivityWarningSent = false; // Reset warning flag
            console.log('[Inactivity] âŒ¨ï¸ User text activity detected, timer reset');
            
            // Add to transcript
            const studentTextEntry: TranscriptEntry = {
              speaker: "student",
              text: message.message,
              timestamp: new Date().toISOString(),
              messageId: crypto.randomUUID(),
            };
            state.transcript.push(studentTextEntry);
            
            // Send transcript update to client
            ws.send(JSON.stringify({
              type: "transcript",
              speaker: "student",
              text: message.message
            }));
            
            // Check content moderation
            try {
              const moderation = await moderateContent(message.message);
              
              if (!moderation.isAppropriate) {
                console.warn('[Custom Voice] âš ï¸ Inappropriate text content detected');
                
                // Send warning response
                const warningText = getModerationResponse('first');
                const warningEntry: TranscriptEntry = {
                  speaker: "tutor",
                  text: warningText,
                  timestamp: new Date().toISOString(),
                  messageId: crypto.randomUUID(),
                };
                state.transcript.push(warningEntry);
                
                ws.send(JSON.stringify({
                  type: "transcript",
                  speaker: "tutor",
                  text: warningText
                }));
                
                // Generate and send warning audio
                const warningAudio = await generateSpeech(warningText, state.ageGroup, state.speechSpeed);
                ws.send(JSON.stringify({
                  type: "audio",
                  data: warningAudio.toString("base64")
                }));
                
                // Record violation
                if (moderation.violationType && state.sessionId) {
                  await db.insert(contentViolations).values({
                    userId: state.userId,
                    sessionId: state.sessionId,
                    violationType: moderation.violationType,
                    severity: moderation.severity,
                    userMessage: message.message
                  });
                }
                
                break; // Don't process further
              }
              
              // Content approved - generate AI response (text input) with STREAMING
              // LANGUAGE: For text input, use detected language if speaking detected it,
              // otherwise use selected language
              const textResponseLanguage = state.detectedLanguage || state.language;
              
              // Mark tutor as speaking for barge-in detection
              state.isTutorSpeaking = true;
              state.lastAudioSentAt = Date.now();
              
              // Track streaming metrics
              let textSentenceCount = 0;
              let textTotalAudioBytes = 0;
              const textStreamStart = Date.now();
              
              // Use streaming with sentence-by-sentence TTS for text input too
              await new Promise<void>((textResolve, textReject) => {
                const textCallbacks: StreamingCallbacks = {
                  onSentence: async (sentence: string) => {
                    textSentenceCount++;
                    console.log(`[Custom Voice] ğŸ“¤ Text sentence ${textSentenceCount}: "${sentence.substring(0, 50)}..."`);
                    
                    if (textSentenceCount === 1) {
                      // Send first sentence with partial flag
                      ws.send(JSON.stringify({
                        type: "transcript",
                        speaker: "tutor",
                        text: sentence,
                        isPartial: true,
                      }));
                    } else {
                      // Update transcript with accumulated text
                      ws.send(JSON.stringify({
                        type: "transcript_update",
                        speaker: "tutor",
                        text: sentence,
                      }));
                    }
                    
                    // Generate TTS for this sentence immediately
                    if (state.tutorAudioEnabled) {
                      try {
                        const audioBuffer = await generateSpeech(sentence, state.ageGroup, state.speechSpeed);
                        textTotalAudioBytes += audioBuffer.length;
                        
                        console.log(`[Custom Voice] ğŸ”Š Text sentence ${textSentenceCount} TTS: ${audioBuffer.length} bytes`);
                        
                        // Send audio chunk immediately
                        ws.send(JSON.stringify({
                          type: "audio",
                          data: audioBuffer.toString("base64"),
                          mimeType: "audio/pcm;rate=16000",
                          isChunk: true,
                          chunkIndex: textSentenceCount,
                        }));
                      } catch (ttsError) {
                        console.error(`[Custom Voice] âŒ Text TTS error for sentence ${textSentenceCount}:`, ttsError);
                      }
                    }
                  },
                  
                  onComplete: (fullText: string) => {
                    const textStreamMs = Date.now() - textStreamStart;
                    console.log(`[Custom Voice] â±ï¸ Text streaming complete: ${textStreamMs}ms, ${textSentenceCount} sentences`);
                    console.log(`[Custom Voice] ğŸ¤– Tutor (text): "${fullText}"`);
                    
                    // Add to conversation history
                    state.conversationHistory.push(
                      { role: "user", content: message.message },
                      { role: "assistant", content: fullText }
                    );
                    
                    // Add AI response to transcript (internal state)
                    const tutorTextEntry: TranscriptEntry = {
                      speaker: "tutor",
                      text: fullText,
                      timestamp: new Date().toISOString(),
                      messageId: crypto.randomUUID(),
                    };
                    state.transcript.push(tutorTextEntry);
                    
                    // Send final complete transcript
                    ws.send(JSON.stringify({
                      type: "transcript",
                      speaker: "tutor",
                      text: fullText,
                      isComplete: true,
                    }));
                    
                    textResolve();
                  },
                  
                  onError: (error: Error) => {
                    console.error("[Custom Voice] âŒ Text streaming error:", error);
                    textReject(error);
                  }
                };
                
                generateTutorResponseStreaming(
                  state.conversationHistory,
                  message.message,
                  state.uploadedDocuments,
                  textCallbacks,
                  state.systemInstruction,
                  "text", // Student typed via chat
                  textResponseLanguage
                ).catch(textReject); // Ensure errors are properly propagated
              });
              
              console.log(`[Custom Voice] ğŸ”Š Sent streamed tutor voice response (${textSentenceCount} chunks)`);
              
              // Reset tutor speaking state after streaming completes
              state.isTutorSpeaking = false;
              
            } catch (error) {
              console.error('[Custom Voice] Error processing text message:', error);
            }
            break;

          case "document_uploaded":
            // Handle document uploaded during session
            console.log(`[Custom Voice] ğŸ“„ Document uploaded during session: ${message.filename}`);
            
            try {
              // Fetch document with chunks from database
              const document = await storage.getDocument(message.documentId, state.userId);
              
              if (document) {
                // Fetch chunks separately
                const chunks = await db
                  .select()
                  .from(documentChunks)
                  .where(eq(documentChunks.documentId, message.documentId))
                  .orderBy(documentChunks.chunkIndex);
                
                if (chunks && chunks.length > 0) {
                  // Format document content
                  const documentContent = `[Document: ${message.filename}]\n${chunks.map((chunk: { content: string }) => chunk.content).join('\n')}`;
                  
                  // Add to session's uploaded documents
                  state.uploadedDocuments.push(documentContent);
                  
                  console.log(`[Custom Voice] âœ… Added document to session context (${chunks.length} chunks)`);
                  
                  // Send acknowledgment via voice
                  const ackMessage = `Great! I can now see "${message.filename}". What would you like to know about it?`;
                  
                  // Add to transcript
                  const ackEntry: TranscriptEntry = {
                    speaker: "tutor",
                    text: ackMessage,
                    timestamp: new Date().toISOString(),
                    messageId: crypto.randomUUID(),
                  };
                  state.transcript.push(ackEntry);
                  
                  // Send transcript update
                  ws.send(JSON.stringify({
                    type: "transcript",
                    speaker: "tutor",
                    text: ackMessage
                  }));
                  
                  // Generate and send voice acknowledgment
                  const ackAudio = await generateSpeech(ackMessage, state.ageGroup, state.speechSpeed);
                  ws.send(JSON.stringify({
                    type: "audio",
                    data: ackAudio.toString("base64")
                  }));
                  
                  console.log(`[Custom Voice] ğŸ”Š Sent document acknowledgment`);
                } else {
                  console.error(`[Custom Voice] Document has no chunks: ${message.documentId}`);
                }
              } else {
                console.error(`[Custom Voice] Document not found: ${message.documentId}`);
              }
            } catch (error) {
              console.error('[Custom Voice] Error adding document to session:', error);
            }
            break;

          case "speech_detected":
            // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            // BARGE-IN: Handle client-side VAD speech detection
            // Client already validated this is real user speech (not echo)
            // so we trust this signal and update server state
            // â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            const timeSinceAudioForVAD = Date.now() - state.lastAudioSentAt;

            // Only process if tutor was speaking recently
            if (state.isTutorSpeaking && timeSinceAudioForVAD < 30000) {
              console.log(`[Custom Voice] ğŸ›‘ BARGE-IN via client VAD (audio sent ${timeSinceAudioForVAD}ms ago)`);

              // Mark interruption for post-interrupt buffer
              state.wasInterrupted = true;
              state.lastInterruptionTime = Date.now();
              state.isTutorSpeaking = false;

              // Send interrupt signal - confirms server is in sync with client
              ws.send(JSON.stringify({
                type: "interrupt",
                message: "Student speaking (VAD)",
              }));

              console.log("[Custom Voice] âœ… VAD barge-in processed");
            } else if (timeSinceAudioForVAD < 30000) {
              // Audio was sent recently but isTutorSpeaking is false
              // This means client already stopped playback, just sync state
              console.log(`[Custom Voice] â„¹ï¸ speech_detected received (tutor not speaking, syncing state)`);
              state.isTutorSpeaking = false;
            }
            break;

          case "update_mode":
            // Handle communication mode updates (voice, hybrid, text-only)
            console.log("[Custom Voice] ğŸ”„ Updating mode:", {
              tutorAudio: message.tutorAudio,
              studentMic: message.studentMic
            });
            
            // Update state
            state.tutorAudioEnabled = message.tutorAudio ?? true;
            state.studentMicEnabled = message.studentMic ?? true;
            
            // Send acknowledgment
            ws.send(JSON.stringify({
              type: "mode_updated",
              tutorAudio: state.tutorAudioEnabled,
              studentMic: state.studentMicEnabled
            }));
            
            console.log("[Custom Voice] âœ… Mode updated:", {
              tutorAudio: state.tutorAudioEnabled ? 'enabled' : 'muted',
              studentMic: state.studentMicEnabled ? 'enabled' : 'muted'
            });
            break;
          
          case "end":
            console.log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
            console.log("[Session End] ğŸ›‘ RECEIVED SESSION END REQUEST");
            console.log("[Session End] Session ID:", state.sessionId);
            console.log("[Session End] User ID:", state.userId);
            console.log("[Session End] Transcript length:", state.transcript.length);
            console.log("[Session End] Session already ended?", state.isSessionEnded);
            console.log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

            // Close Deepgram connection first
            if (state.deepgramConnection) {
              console.log("[Session End] ğŸ¤ Closing Deepgram connection...");
              state.deepgramConnection.close();
              state.deepgramConnection = null;
              console.log("[Session End] âœ… Deepgram closed");
            }

            // Clear persistence interval
            console.log("[Session End] ğŸ§¹ Clearing persistence interval...");
            clearInterval(persistInterval);
            console.log("[Session End] âœ… Persistence interval cleared");

            // Finalize session (saves to DB, deducts minutes)
            console.log("[Session End] ğŸ’¾ Calling finalizeSession...");
            try {
              await finalizeSession(state, 'normal');
              console.log("[Session End] âœ… finalizeSession completed successfully");
            } catch (error) {
              console.error("[Session End] âŒ finalizeSession FAILED:", error);
              // Don't throw - still try to close gracefully
            }

            // Send acknowledgment to client
            console.log("[Session End] ğŸ“¤ Sending session_ended ACK to client...");
            ws.send(JSON.stringify({ 
              type: "session_ended",
              sessionId: state.sessionId,
              transcriptLength: state.transcript.length,
              success: true
            }));
            console.log("[Session End] âœ… ACK sent");
            
            // Close WebSocket
            console.log("[Session End] ğŸ”Œ Closing WebSocket...");
            ws.close(1000, 'Session ended normally');
            console.log("[Session End] âœ… Session end complete");
            console.log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
            break;

          default:
            console.warn("[Custom Voice] âš ï¸ Unknown message type:", message.type);
        }
      } catch (error) {
        console.error("[Custom Voice] âŒ Error handling message:", error);
        ws.send(JSON.stringify({ 
          type: "error", 
          error: error instanceof Error ? error.message : "Unknown error"
        }));
      }
    });

    ws.on("close", async () => {
      console.log("[Custom Voice] ğŸ”Œ Connection closed");
      
      // Skip if session was already ended (prevents double-deduction)
      if (state.isSessionEnded) {
        console.log("[Custom Voice] â„¹ï¸ Session already finalized, skipping close handler");
        return;
      }
      
      // Clear response timer
      if (responseTimer) {
        clearTimeout(responseTimer);
        responseTimer = null;
      }
      
      // Close Deepgram first
      if (state.deepgramConnection) {
        state.deepgramConnection.close();
        state.deepgramConnection = null;
      }
      
      // Clear persistence interval (inactivity timer cleared in finalizeSession)
      clearInterval(persistInterval);
      
      // Finalize session (saves to DB, deducts minutes)
      await finalizeSession(state, 'disconnect');
    });

    ws.on("error", async (error) => {
      console.error("[Custom Voice] âŒ WebSocket error:", error);
      
      // Skip if session was already ended (prevents double-deduction)
      if (state.isSessionEnded) {
        console.log("[Custom Voice] â„¹ï¸ Session already finalized, skipping error handler");
        return;
      }
      
      // Close Deepgram first
      if (state.deepgramConnection) {
        state.deepgramConnection.close();
        state.deepgramConnection = null;
      }
      
      // Clear intervals before finalizing (inactivity timer cleared in finalizeSession)
      clearInterval(persistInterval);
      if (responseTimer) {
        clearTimeout(responseTimer);
        responseTimer = null;
      }
      
      // Finalize session with error message (saves to DB, deducts minutes)
      await finalizeSession(state, 'error', error instanceof Error ? error.message : 'Unknown error');
    });
  });

  return wss;
}