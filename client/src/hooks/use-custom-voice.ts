import { useState, useRef, useCallback, useEffect } from "react";

interface TranscriptMessage {
  speaker: "student" | "tutor" | "system";
  text: string;
  timestamp?: string;
}

interface MicrophoneError {
  message: string;
  troubleshooting: string[];
  errorType: string;
}

export function useCustomVoice() {
  const [isConnected, setIsConnected] = useState(false);
  const [transcript, setTranscript] = useState<TranscriptMessage[]>([]);
  const [error, setError] = useState<string | null>(null);
  const [microphoneError, setMicrophoneError] = useState<MicrophoneError | null>(null);
  const [isTutorSpeaking, setIsTutorSpeaking] = useState(false);
  const [audioEnabled, setAudioEnabled] = useState(true);
  const [micEnabled, setMicEnabled] = useState(true);
  
  const wsRef = useRef<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const processorRef = useRef<AudioWorkletNode | null>(null);
  const audioQueueRef = useRef<AudioBuffer[]>([]);
  const isPlayingRef = useRef(false);
  const currentAudioSourceRef = useRef<AudioBufferSourceNode | null>(null);
  const audioEnabledRef = useRef<boolean>(true); // Tutor audio enabled (default true)
  const micEnabledRef = useRef<boolean>(true); // Student mic enabled (default true)
  const playbackGainNodeRef = useRef<GainNode | null>(null); // For smooth fadeout during playback
  const nextPlayTimeRef = useRef<number>(0); // Schedule next chunk seamlessly
  const scheduledSourcesRef = useRef<AudioBufferSourceNode[]>([]); // Track scheduled sources for cleanup

  // Track when tutor audio playback started to prevent self-interrupt
  // VAD will ignore speech detection for a short period after playback starts
  const lastAudioPlaybackStartRef = useRef<number>(0);
  const isTutorSpeakingRef = useRef<boolean>(false); // Ref version for audio worklet access
  
  // Track if stream cleanup has been triggered to prevent spam logging
  const streamCleanupTriggeredRef = useRef<boolean>(false);
  
  // Track auto-recovery for unexpected audio track deaths
  // Uses a Promise-based mutex to serialize recovery attempts
  const recoveryPromiseRef = useRef<Promise<void> | null>(null);
  const selectedMicrophoneIdRef = useRef<string | null>(null);  // Store original device ID
  const selectedMicrophoneLabelRef = useRef<string | null>(null);  // Store device label as backup
  const MAX_MIC_RECOVERY_ATTEMPTS = 3;
  const MIC_RECOVERY_DELAY_MS = 500;
  
  // Device exclusion patterns - never select these as fallback
  const EXCLUDED_DEVICE_PATTERNS = [
    'stereo mix',
    'what u hear',
    'wave out',
    'loopback',
    'virtual',
    'cable'
  ];
  
  // Check if user allows virtual audio devices
  const getAllowVirtualAudio = (): boolean => {
    try {
      return localStorage.getItem('jie-allow-virtual-audio') === 'true';
    } catch {
      return false;
    }
  };

  // Find best microphone by filtering out system audio devices (respects user's virtual audio preference)
  const findBestMicrophone = async (): Promise<string | null> => {
    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const microphones = devices.filter(d => d.kind === 'audioinput');
      const allowVirtual = getAllowVirtualAudio();
      
      // Filter out system audio devices unless user explicitly enabled virtual devices
      const realMics = microphones.filter(mic => {
        if (allowVirtual) return true;
        const label = mic.label.toLowerCase();
        return !EXCLUDED_DEVICE_PATTERNS.some(pattern => label.includes(pattern));
      });
      
      if (realMics.length > 0) {
        console.log(`[Custom Voice] üé§ Found ${realMics.length} real microphone(s) after filtering (allowVirtual=${allowVirtual})`);
        return realMics[0].deviceId;
      }
      
      return null;
    } catch (error) {
      console.error('[Custom Voice] ‚ùå Error finding best microphone:', error);
      return null;
    }
  };
  
  // Find microphone by label - helps when device IDs change between sessions
  const findMicrophoneByLabel = async (label: string): Promise<string | null> => {
    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const microphones = devices.filter(d => d.kind === 'audioinput');
      
      // Exact match
      const exactMatch = microphones.find(m => m.label === label);
      if (exactMatch) {
        console.log(`[Custom Voice] üé§ Found microphone by exact label match: ${label}`);
        return exactMatch.deviceId;
      }
      
      // Partial match (first few characters)
      const partialMatch = microphones.find(m => 
        m.label.toLowerCase().includes(label.substring(0, 10).toLowerCase())
      );
      if (partialMatch) {
        console.log(`[Custom Voice] üé§ Found microphone by partial label match: ${partialMatch.label}`);
        return partialMatch.deviceId;
      }
      
      return null;
    } catch (error) {
      console.error('[Custom Voice] ‚ùå Error finding microphone by label:', error);
      return null;
    }
  };
  
  // Cleanup helper - safely cleans up mic resources
  // Sets streamCleanupTriggeredRef BEFORE stopping to prevent onended from triggering recovery
  const cleanupMicResources = () => {
    // CRITICAL: Set flag BEFORE stopping tracks to prevent onended from triggering false recovery
    streamCleanupTriggeredRef.current = true;
    
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop());
      mediaStreamRef.current = null;
    }
    if (processorRef.current) {
      try { processorRef.current.disconnect(); } catch (e) { /* ignore */ }
      processorRef.current = null;
    }
    
    // Reset flag after cleanup is complete - new startMicrophone will set it to false
  };
  
  // Dedicated recovery function with multi-stage retry strategy
  // Stage 1: Exact deviceId (same device)
  // Stage 2: Match by label (device label changed)
  // Stage 3: Filter & pick best microphone (avoid Stereo Mix)
  const attemptMicRecovery = async () => {
    // Don't recover if mic is intentionally disabled
    if (!micEnabledRef.current) {
      console.log('[Custom Voice] ‚ÑπÔ∏è Mic disabled, skipping recovery');
      return;
    }
    
    // If recovery is already in progress, wait for it to complete
    if (recoveryPromiseRef.current) {
      console.log('[Custom Voice] ‚ÑπÔ∏è Recovery already in progress, waiting...');
      try {
        await recoveryPromiseRef.current;
      } catch (e) { /* ignore */ }
      // After waiting, check if stream is now healthy
      const currentStream = mediaStreamRef.current as MediaStream | null;
      if (currentStream && currentStream.active) {
        console.log('[Custom Voice] ‚ÑπÔ∏è Stream already recovered by previous attempt');
        return;
      }
      // Otherwise fall through to start a new recovery
    }
    
    // Start new recovery - create and store the promise
    const recoveryPromise = (async () => {
      let lastError: unknown = null;
      
      for (let attempt = 1; attempt <= MAX_MIC_RECOVERY_ATTEMPTS; attempt++) {
        console.log(`[Custom Voice] üîÑ Auto-recovering microphone (attempt ${attempt}/${MAX_MIC_RECOVERY_ATTEMPTS})...`);
        
        // Clean up old resources first (sets streamCleanupTriggeredRef to prevent false triggers)
        cleanupMicResources();
        
        // Stage 1: Try exact device ID first
        if (selectedMicrophoneIdRef.current && attempt === 1) {
          const delayMs = 500;
          console.log(`[Custom Voice] ‚è≥ Waiting ${delayMs}ms for device to stabilize...`);
          await new Promise(resolve => setTimeout(resolve, delayMs));
          
          try {
            console.log(`[Custom Voice] üéØ Attempt ${attempt}: Trying exact deviceId`);
            const stream = await navigator.mediaDevices.getUserMedia({
              audio: {
                deviceId: { exact: selectedMicrophoneIdRef.current },
                sampleRate: 16000,
                channelCount: 1,
                echoCancellation: true,
                noiseSuppression: false,
                autoGainControl: true,
              }
            });
            
            // Setup stream and exit recovery
            mediaStreamRef.current = stream;
            setupAudioTrackListener(stream);
            setMicrophoneError(null);
            console.log('[Custom Voice] ‚úÖ Recovered with exact deviceId');
            return;
          } catch (e) {
            console.warn(`[Custom Voice] ‚ö†Ô∏è Exact deviceId failed: ${(e as Error).message}`);
          }
        }
        
        // Stage 2: Try matching by label (if we stored it)
        if (selectedMicrophoneLabelRef.current && attempt === 2) {
          const delayMs = 1000;
          console.log(`[Custom Voice] ‚è≥ Waiting ${delayMs}ms before label-match attempt...`);
          await new Promise(resolve => setTimeout(resolve, delayMs));
          
          try {
            const matchedDeviceId = await findMicrophoneByLabel(selectedMicrophoneLabelRef.current);
            if (matchedDeviceId) {
              console.log(`[Custom Voice] üéØ Attempt ${attempt}: Trying label match`);
              const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                  deviceId: { exact: matchedDeviceId },
                  sampleRate: 16000,
                  channelCount: 1,
                  echoCancellation: true,
                  noiseSuppression: false,
                  autoGainControl: true,
                }
              });
              
              // Update stored ID and setup
              selectedMicrophoneIdRef.current = matchedDeviceId;
              mediaStreamRef.current = stream;
              setupAudioTrackListener(stream);
              setMicrophoneError(null);
              console.log('[Custom Voice] ‚úÖ Recovered with label match');
              return;
            }
          } catch (e) {
            console.warn(`[Custom Voice] ‚ö†Ô∏è Label match failed: ${(e as Error).message}`);
          }
        }
        
        // Stage 3: Fall back to best microphone (filtered list)
        if (attempt >= 2) {
          const delayMs = 1500;
          console.log(`[Custom Voice] ‚è≥ Waiting ${delayMs}ms before filtered fallback...`);
          await new Promise(resolve => setTimeout(resolve, delayMs));
          
          try {
            const bestMicId = await findBestMicrophone();
            if (bestMicId) {
              console.log(`[Custom Voice] üéØ Attempt ${attempt}: Trying filtered fallback (${bestMicId})`);
              const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                  deviceId: { exact: bestMicId },
                  sampleRate: 16000,
                  channelCount: 1,
                  echoCancellation: true,
                  noiseSuppression: false,
                  autoGainControl: true,
                }
              });
              
              // Update stored info and setup
              selectedMicrophoneIdRef.current = bestMicId;
              mediaStreamRef.current = stream;
              setupAudioTrackListener(stream);
              setMicrophoneError(null);
              console.log('[Custom Voice] ‚úÖ Recovered with filtered fallback');
              return;
            }
          } catch (e) {
            console.warn(`[Custom Voice] ‚ö†Ô∏è Filtered fallback failed: ${(e as Error).message}`);
          }
        }
        
        // Give it time before next attempt
        if (attempt < MAX_MIC_RECOVERY_ATTEMPTS) {
          const nextDelayMs = 500 * (attempt + 1);
          console.log(`[Custom Voice] ‚è≥ Waiting ${nextDelayMs}ms before next recovery attempt...`);
          await new Promise(resolve => setTimeout(resolve, nextDelayMs));
        }
      }
      
      // All attempts exhausted - ALWAYS show error to user
      console.error('[Custom Voice] ‚ùå All recovery attempts failed, last error:', lastError);
      setMicrophoneError({
        message: 'Microphone connection lost',
        troubleshooting: [
          'Click the microphone icon to retry',
          'Check if another app is using your microphone',
          'Try refreshing the page'
        ],
        errorType: 'TRACK_ENDED'
      });
    })();
    
    recoveryPromiseRef.current = recoveryPromise;
    
    try {
      await recoveryPromise;
    } finally {
      // Clear the promise only if it's still the current one
      if (recoveryPromiseRef.current === recoveryPromise) {
        recoveryPromiseRef.current = null;
      }
    }
  };
  
  // Manual retry helper - waits for any active recovery, then starts fresh
  const forceStartMicrophone = async () => {
    console.log('[Custom Voice] üîÑ Manual microphone retry requested');
    
    // Wait for any active recovery to complete first
    if (recoveryPromiseRef.current) {
      console.log('[Custom Voice] ‚ÑπÔ∏è Waiting for active recovery to complete...');
      try {
        await recoveryPromiseRef.current;
      } catch (e) { /* ignore */ }
    }
    
    // Now start fresh
    cleanupMicResources();
    setMicrophoneError(null);
    await startMicrophone();
  };

  // Synchronize refs with state
  useEffect(() => {
    audioEnabledRef.current = audioEnabled;
  }, [audioEnabled]);

  useEffect(() => {
    micEnabledRef.current = micEnabled;
  }, [micEnabled]);

  useEffect(() => {
    isTutorSpeakingRef.current = isTutorSpeaking;
  }, [isTutorSpeaking]);

  const connect = useCallback(async (
    sessionId: string, 
    userId: string,
    studentName: string,
    ageGroup: string,
    systemInstruction: string,
    documents: string[] = [],
    language: string = 'en'
  ) => {
    try {
      console.log("[Custom Voice] üöÄ Connecting...", { language });
      
      // Get WebSocket URL (use wss:// in production)
      const protocol = window.location.protocol === "https:" ? "wss:" : "ws:";
      const wsUrl = `${protocol}//${window.location.host}/api/custom-voice-ws`;
      
      const ws = new WebSocket(wsUrl);
      wsRef.current = ws;

      ws.onopen = () => {
        console.log("[Custom Voice] ‚úÖ Connected");
        
        ws.send(JSON.stringify({
          type: "init",
          sessionId,
          userId,
          studentName,
          ageGroup,
          systemInstruction,
          documents,
          language, // LANGUAGE: Pass selected language to backend
        }));
      };

      ws.onmessage = async (event) => {
        const message = JSON.parse(event.data);

        switch (message.type) {
          case "ready":
            console.log("[Custom Voice] ‚úÖ Session ready");
            setIsConnected(true);
            
            // Only start microphone if student mic is enabled
            if (micEnabledRef.current) {
              console.log("[Custom Voice] üé§ Starting microphone (Voice mode)");
              await startMicrophone();
            } else {
              console.log("[Custom Voice] üîá Skipping microphone (Hybrid/Text mode)");
            }
            break;

          case "transcript":
            console.log(`[Custom Voice] üìù ${message.speaker}: ${message.text}`);
            // Handle streaming transcripts: isPartial = first chunk, isComplete = final
            if (message.isComplete) {
              // Replace partial transcript with final complete version
              setTranscript(prev => {
                const updated = [...prev];
                const lastIdx = updated.length - 1;
                if (lastIdx >= 0 && updated[lastIdx].speaker === 'tutor') {
                  updated[lastIdx] = {
                    speaker: message.speaker,
                    text: message.text,
                    timestamp: new Date().toISOString(),
                  };
                  return updated;
                }
                return [...prev, {
                  speaker: message.speaker,
                  text: message.text,
                  timestamp: new Date().toISOString(),
                }];
              });
            } else {
              // New transcript entry (or partial first chunk)
              setTranscript(prev => [...prev, {
                speaker: message.speaker,
                text: message.text,
                timestamp: new Date().toISOString(),
              }]);
            }
            break;
          
          case "transcript_update":
            // Streaming: append text to last tutor transcript entry
            console.log(`[Custom Voice] üìù +${message.speaker}: ${message.text.substring(0, 30)}...`);
            setTranscript(prev => {
              const updated = [...prev];
              const lastIdx = updated.length - 1;
              if (lastIdx >= 0 && updated[lastIdx].speaker === 'tutor') {
                updated[lastIdx] = {
                  ...updated[lastIdx],
                  text: updated[lastIdx].text + ' ' + message.text,
                };
                return updated;
              }
              return prev;
            });
            break;

          case "audio":
            // Log streaming metadata for debugging
            const audioBytes = message.data?.length || 0;
            const isChunk = message.isChunk || false;
            const chunkIdx = message.chunkIndex || 0;
            console.log(`[Custom Voice] üîä Received audio: ${audioBytes} chars (isChunk=${isChunk}, chunkIndex=${chunkIdx})`);
            
            if (audioEnabled) {
              console.log("[Custom Voice] üîä Playing audio chunk");
              // Record when playback starts to prevent self-interrupt from echo
              lastAudioPlaybackStartRef.current = Date.now();
              setIsTutorSpeaking(true);
              await playAudio(message.data);
            } else {
              console.log("[Custom Voice] üîá Audio muted, showing text only");
            }
            break;

          case "interrupt":
            console.log("[Custom Voice] üõë Interruption detected - stopping tutor");
            stopAudio();
            setIsTutorSpeaking(false);
            break;
          
          case "mode_updated":
            console.log("[Custom Voice] Mode synced:", {
              tutorAudio: message.tutorAudio,
              studentMic: message.studentMic
            });
            setAudioEnabled(message.tutorAudio);
            setMicEnabled(message.studentMic);
            break;

          case "error":
            console.error("[Custom Voice] ‚ùå Error:", message.error);
            setError(message.error);
            break;

          case "ended":
            console.log("[Custom Voice] ‚úÖ Session ended (deprecated message)");
            break;
          
          case "session_ended":
            console.log("[Custom Voice] ‚úÖ Received session_ended ACK from server");
            console.log("[Custom Voice] Session ID:", message.sessionId);
            console.log("[Custom Voice] Reason:", message.reason);
            console.log("[Custom Voice] Transcript length:", message.transcriptLength);
            
            // Show notification if session ended due to inactivity
            if (message.reason === 'inactivity_timeout') {
              console.log("[Custom Voice] üîî Session ended due to inactivity - will show notification");
              // Store inactivity flag so parent component can show notification
              (window as any).__sessionEndedReason = 'inactivity_timeout';
            }
            
            // Cleanup is handled in ws.onclose
            break;
        }
      };

      ws.onerror = (error) => {
        console.error("[Custom Voice] ‚ùå WebSocket error:", error);
        setError("Connection error");
      };

      ws.onclose = () => {
        console.log("[Custom Voice] üîå Disconnected");
        setIsConnected(false);
        cleanup();
      };

    } catch (error) {
      console.error("[Custom Voice] ‚ùå Connection failed:", error);
      setError(error instanceof Error ? error.message : "Connection failed");
    }
  }, []);

  // Helper to setup track listener - extracted so recovery can also use it
  const setupAudioTrackListener = (stream: MediaStream) => {
    const audioTrack = stream.getAudioTracks()[0];
    if (audioTrack) {
      // Store device ID AND label from successful connection for recovery
      const settings = audioTrack.getSettings();
      selectedMicrophoneIdRef.current = settings.deviceId || null;
      selectedMicrophoneLabelRef.current = audioTrack.label || null;
      console.log(`[Custom Voice] üé§ Using microphone: ${audioTrack.label}, deviceId: ${selectedMicrophoneIdRef.current || 'unknown'}`);
      
      audioTrack.onended = () => {
        // Skip if this was an intentional cleanup
        if (streamCleanupTriggeredRef.current) {
          console.log('[Custom Voice] ‚ÑπÔ∏è Audio track ended (intentional cleanup)');
          return;
        }
        
        console.error('[Custom Voice] ‚ö†Ô∏è Audio track ended unexpectedly');
        console.error('[Custom Voice] Track state:', audioTrack.readyState);
        console.error('[Custom Voice] Track enabled:', audioTrack.enabled);
        console.error('[Custom Voice] Track muted:', audioTrack.muted);
        attemptMicRecovery(); // Async recovery with multi-stage retry
      };
      console.log('[Custom Voice] üì° Added track.onended listener for track:', audioTrack.label);
    }
  };

  // Helper to get preferred microphone from settings
  const getPreferredMicrophoneId = async (): Promise<string | null> => {
    try {
      const preferredId = localStorage.getItem('jie-preferred-microphone-id');
      const preferredLabel = localStorage.getItem('jie-preferred-microphone-label');
      
      // If no preference set or explicitly system-default, return null
      if (!preferredId) {
        return null;
      }
      
      // Try to find device by ID first
      const devices = await navigator.mediaDevices.enumerateDevices();
      const byId = devices.find(d => d.kind === 'audioinput' && d.deviceId === preferredId);
      if (byId) {
        console.log('[Custom Voice] üéØ Found preferred mic by ID:', byId.label);
        return byId.deviceId;
      }
      
      // Try to find device by label (IDs can change between sessions)
      if (preferredLabel) {
        const byLabel = devices.find(d => d.kind === 'audioinput' && d.label === preferredLabel);
        if (byLabel) {
          console.log('[Custom Voice] üéØ Found preferred mic by label:', byLabel.label);
          return byLabel.deviceId;
        }
      }
      
      console.log('[Custom Voice] ‚ÑπÔ∏è Preferred mic not found, using system default');
      return null;
    } catch (e) {
      console.warn('[Custom Voice] ‚ö†Ô∏è Error getting preferred mic:', e);
      return null;
    }
  };

  const startMicrophone = async () => {
    try {
      console.log("[Custom Voice] üé§ Requesting microphone access...");
      
      // Reset cleanup flag for new microphone session
      streamCleanupTriggeredRef.current = false;
      
      // Check if browser supports getUserMedia
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        console.error('[Custom Voice] ‚ùå Browser does not support getUserMedia');
        setMicrophoneError({
          type: 'not-supported',
          message: 'Your browser does not support voice recording. Please use a modern browser like Chrome, Firefox, or Edge.',
          canRetry: false,
        });
        throw new Error('BROWSER_NOT_SUPPORTED');
      }
      
      // First check for user's preferred microphone from settings
      const preferredMicId = await getPreferredMicrophoneId();
      
      // Priority: 1) User preference from settings, 2) Recovery deviceId, 3) System default
      const targetDeviceId = preferredMicId || selectedMicrophoneIdRef.current;
      
      // Build constraints: use the target device if we have one
      const audioConstraints: MediaStreamConstraints['audio'] = targetDeviceId
        ? {
            deviceId: { exact: targetDeviceId },
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: false,  // Disable - can cut off quiet speech
            autoGainControl: true,    // Let browser boost quiet audio
          }
        : {
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: false,  // Disable - can cut off quiet speech
            autoGainControl: true,    // Let browser boost quiet audio
          };
      
      let stream: MediaStream;
      try {
        stream = await navigator.mediaDevices.getUserMedia({ audio: audioConstraints });
      } catch (error) {
        // If exact device ID constraint failed (device disconnected?), try any device
        if (targetDeviceId) {
          console.warn('[Custom Voice] ‚ö†Ô∏è Failed to use preferred/recovery device (ID:', targetDeviceId, '), trying any device...');
          selectedMicrophoneIdRef.current = null; // Reset device ID
          stream = await navigator.mediaDevices.getUserMedia({
            audio: {
              sampleRate: 16000,
              channelCount: 1,
              echoCancellation: true,
              noiseSuppression: false,
              autoGainControl: true,
            }
          });
        } else {
          throw error;
        }
      }
      
      console.log("[Custom Voice] ‚úÖ Microphone access granted");
      
      // Clear any previous errors
      setMicrophoneError(null);
      
      // Sync the actual device ID/label to localStorage for preference persistence
      const audioTrack = stream.getAudioTracks()[0];
      if (audioTrack) {
        const settings = audioTrack.getSettings();
        const actualDeviceId = settings.deviceId;
        if (actualDeviceId) {
          selectedMicrophoneIdRef.current = actualDeviceId;
          selectedMicrophoneLabelRef.current = audioTrack.label;
          
          // Only update localStorage if user had a preference set (not system default)
          const hadPreference = localStorage.getItem('jie-preferred-microphone-id');
          if (hadPreference) {
            localStorage.setItem('jie-preferred-microphone-id', actualDeviceId);
            localStorage.setItem('jie-preferred-microphone-label', audioTrack.label);
            console.log('[Custom Voice] üîÑ Synced mic preference:', audioTrack.label);
          }
        }
      }
      
      mediaStreamRef.current = stream;
      
      // Setup track listener with new helper
      setupAudioTrackListener(stream);
      
      // CRITICAL: Reuse existing AudioContext if it exists (created by playAudio)
      // Creating a new one would orphan gain nodes and scheduled sources from playback
      if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
        audioContextRef.current = new AudioContext({ sampleRate: 16000 });
        console.log("[Custom Voice] üîä Created new AudioContext for microphone");
      } else {
        console.log("[Custom Voice] üîä Reusing existing AudioContext");
      }
      
      // Resume audio context if suspended
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
        console.log("[Custom Voice] ‚úÖ Audio context resumed from suspended state");
      }
      
      try {
        // Load AudioWorklet processor (modern API, replaces deprecated ScriptProcessorNode)
        await audioContextRef.current.audioWorklet.addModule('/audio-processor.js');

        const source = audioContextRef.current.createMediaStreamSource(stream);

        // Add a GainNode to amplify quiet microphones at the hardware level
        const gainNode = audioContextRef.current.createGain();
        gainNode.gain.value = 3.0; // 3x amplification before processing

        const processor = new AudioWorkletNode(audioContextRef.current, 'audio-processor');
        processorRef.current = processor;
        
        // VAD state for AudioWorklet path (mirrors ScriptProcessor fallback logic)
        let workletSpeechStartTime = 0;
        let workletLastSpeechEndTime = 0;
        let workletPostInterruptionBufferActive = false;
        let workletPostInterruptionTimeout: ReturnType<typeof setTimeout> | null = null;
        
        const MIN_SPEECH_DURATION_MS = 600;
        const SPEECH_COALESCE_WINDOW_MS = 1000;
        const POST_INTERRUPTION_BUFFER_MS = 2000;
        const SPEECH_DEBOUNCE_MS = 150;
        
        // Handle audio data and VAD events from AudioWorklet
        processor.port.onmessage = (event) => {
          if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return;

          // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          // RESPONSIVE BARGE-IN with ECHO PROTECTION (AudioWorklet)
          // - Lowered thresholds for faster interruption (0.08 RMS, 0.15 peak)
          // - Reduced 300ms cooldown (was 500ms) for snappier response
          // - Post-interruption buffer prevents fragmented speech (Dec 10, 2025)
          // - Minimum speech duration and coalescing for complete utterances
          // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          if (event.data.type === 'speech_start') {
            const now = Date.now();
            
            // If tutor is currently speaking, we need confidence that this is
            // the USER speaking and not just echo from the speakers
            if (isTutorSpeakingRef.current && isPlayingRef.current) {
              const rms = event.data.rms || 0;
              const peak = event.data.peak || 0;
              const timeSincePlayback = now - lastAudioPlaybackStartRef.current;
              
              // Skip VAD for 300ms after tutor audio starts (reduced from 500ms for faster response)
              if (timeSincePlayback < 300) {
                console.log(`[Custom Voice] ‚è±Ô∏è VAD cooldown active (${(300 - timeSincePlayback).toFixed(0)}ms) - ignoring speech`);
                return;
              }

              // Lowered thresholds for more responsive interruption
              // Real user speech should be RMS 0.08+, ambient noise is typically 0.02-0.04
              const BARGE_IN_RMS = 0.08;
              const BARGE_IN_PEAK = 0.15;

              if (rms < BARGE_IN_RMS && peak < BARGE_IN_PEAK) {
                console.log(`[Custom Voice] üîá VAD: Ignoring ambient sound during tutor (rms=${rms.toFixed(4)}, peak=${peak.toFixed(4)}) - below barge-in threshold`);
                return;
              }
              
              // Start debounce timer for sustained speech
              if (workletSpeechStartTime === 0) {
                workletSpeechStartTime = now;
                console.log(`[Custom Voice] üé§ VAD: Speech onset detected (rms=${rms.toFixed(4)}, starting ${SPEECH_DEBOUNCE_MS}ms debounce...)`);
                return;
              }
              
              // Check if speech sustained for debounce period
              if (now - workletSpeechStartTime < SPEECH_DEBOUNCE_MS) {
                console.log(`[Custom Voice] ‚è±Ô∏è VAD debounce: ${(now - workletSpeechStartTime).toFixed(0)}ms - waiting for sustained speech`);
                return;
              }

              console.log(`[Custom Voice] üõë VAD: CONFIRMED barge-in after debounce (rms=${rms.toFixed(4)}, peak=${peak.toFixed(4)})`);
              stopAudio();
              setIsTutorSpeaking(false);
              
              // POST-INTERRUPTION BUFFER (Dec 10, 2025 FIX)
              // After barge-in, ignore rapid speech-end events for 2 seconds
              workletPostInterruptionBufferActive = true;
              if (workletPostInterruptionTimeout) {
                clearTimeout(workletPostInterruptionTimeout);
              }
              workletPostInterruptionTimeout = setTimeout(() => {
                workletPostInterruptionBufferActive = false;
                console.log('[Custom Voice] ‚úÖ Post-interruption buffer ended (AudioWorklet)');
              }, POST_INTERRUPTION_BUFFER_MS);
              console.log(`[Custom Voice] üõ°Ô∏è Post-interruption buffer active for ${POST_INTERRUPTION_BUFFER_MS}ms (AudioWorklet)`);

              // Notify server for state sync
              wsRef.current.send(JSON.stringify({ type: "speech_detected" }));
            } else {
              // Tutor not speaking - track speech start for min duration check
              if (workletSpeechStartTime === 0) {
                workletSpeechStartTime = now;
              }
              console.log("[Custom Voice] üé§ VAD: Speech detected (tutor not playing)");
            }
            return;
          }

          if (event.data.type === 'speech_end') {
            const now = Date.now();
            const speechDuration = workletSpeechStartTime > 0 ? now - workletSpeechStartTime : 0;
            const timeSinceLastEnd = workletLastSpeechEndTime > 0 ? now - workletLastSpeechEndTime : Infinity;
            
            // POST-INTERRUPTION BUFFER: Ignore speech-end during buffer period
            if (workletPostInterruptionBufferActive) {
              console.log(`[Custom Voice] üõ°Ô∏è VAD: Ignoring speech_end during post-interruption buffer (AudioWorklet)`);
              return;
            }
            
            // MIN SPEECH DURATION: Ignore very short utterances (likely noise/hesitation)
            if (speechDuration > 0 && speechDuration < MIN_SPEECH_DURATION_MS) {
              console.log(`[Custom Voice] ‚è±Ô∏è VAD: Speech too short (${speechDuration}ms < ${MIN_SPEECH_DURATION_MS}ms), ignoring (AudioWorklet)`);
              workletSpeechStartTime = 0;
              return;
            }
            
            // SPEECH COALESCING: If speech ended recently, this might be a continuation
            if (timeSinceLastEnd < SPEECH_COALESCE_WINDOW_MS) {
              console.log(`[Custom Voice] üîó VAD: Coalescing speech (${timeSinceLastEnd}ms since last end < ${SPEECH_COALESCE_WINDOW_MS}ms window) (AudioWorklet)`);
              return;
            }
            
            console.log(`[Custom Voice] üîá VAD: Speech ended (duration=${speechDuration}ms) (AudioWorklet)`);
            workletLastSpeechEndTime = now;
            workletSpeechStartTime = 0;
            return;
          }
          // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

          const float32Data = event.data.data; // Float32Array from AudioWorklet

          // Convert Float32 to PCM16 with gain amplification and SOFT LIMITING
          // Note: We already have 3x hardware gain from GainNode, so use moderate software gain
          const GAIN = 10; // Reduced from 30 to prevent clipping (30x total with 3x hardware)
          const SOFT_THRESHOLD = 0.8; // Start soft limiting at 80% of max amplitude
          
          // Soft limiting function - prevents harsh clipping that breaks Deepgram STT
          // Uses tanh-based compression for values above threshold
          const softLimit = (x: number): number => {
            const absX = Math.abs(x);
            if (absX < SOFT_THRESHOLD) return x;
            const sign = x > 0 ? 1 : -1;
            const excess = absX - SOFT_THRESHOLD;
            const headroom = 1.0 - SOFT_THRESHOLD;
            // Smooth compression: excess is compressed using tanh
            const compressed = SOFT_THRESHOLD + headroom * Math.tanh(excess / headroom * 2);
            return sign * Math.min(compressed, 0.98); // Never quite hit 1.0
          };
          
          const pcm16 = new Int16Array(float32Data.length);
          for (let i = 0; i < float32Data.length; i++) {
            const amplified = float32Data[i] * GAIN;
            const limited = softLimit(amplified);
            pcm16[i] = limited < 0 ? limited * 0x8000 : limited * 0x7FFF;
          }

          const uint8Array = new Uint8Array(pcm16.buffer);
          const binaryString = Array.from(uint8Array).map(byte => String.fromCharCode(byte)).join('');

          wsRef.current.send(JSON.stringify({
            type: "audio",
            data: btoa(binaryString),
          }));
        };

        // Connect: source -> gainNode -> processor -> destination
        source.connect(gainNode);
        gainNode.connect(processor);
        processor.connect(audioContextRef.current.destination);

        console.log("[Custom Voice] üîä Audio chain: mic -> gain(3x) -> worklet -> destination");
      } catch (workletError) {
        console.warn('[Custom Voice] ‚ö†Ô∏è AudioWorklet not supported, falling back to ScriptProcessorNode:', workletError);

        // Fallback to ScriptProcessorNode for older browsers
        const source = audioContextRef.current.createMediaStreamSource(stream);

        // Add gain node for fallback path too
        const gainNode = audioContextRef.current.createGain();
        gainNode.gain.value = 3.0; // 3x amplification

        const processor = audioContextRef.current.createScriptProcessor(2048, 1, 1); // Smaller buffer for lower latency
        processorRef.current = processor as any;

        // VAD state for fallback processor
        let speechActive = false;
        let silentChunks = 0;
        let speechStartTime = 0; // Track when speech detected
        let speechEndTime = 0; // Track when silence started
        let lastSpeechEndTime = 0; // Track last confirmed speech end for coalescing
        let postInterruptionBufferActive = false; // Post-barge-in buffer period
        let postInterruptionTimeout: NodeJS.Timeout | null = null; // Timer for buffer period
        
        const MAX_SILENT_CHUNKS = 5; // Only ~100ms of silence before considering speech ended
        const VAD_THRESHOLD = 0.06; // Base speech detection threshold (was 0.003, too low)
        const SPEECH_DEBOUNCE_MS = 150; // Require 150ms of sustained speech to trigger
        const SILENCE_DEBOUNCE_MS = 1200; // Require 1.2s of sustained silence to end (Dec 10, 2025: increased from 800ms for mid-sentence pauses)
        const MIN_SPEECH_DURATION_MS = 600; // Minimum speech duration before considering complete (Dec 10, 2025)
        const SPEECH_COALESCE_WINDOW_MS = 1000; // Coalesce rapid speech events within 1 second
        const POST_INTERRUPTION_BUFFER_MS = 2000; // After barge-in, ignore speech-end events for 2s

        processor.onaudioprocess = (e) => {
          if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return;

          // Check if media stream is still active - trigger recovery if died
          if (!mediaStreamRef.current || !mediaStreamRef.current.active) {
            if (!streamCleanupTriggeredRef.current) {
              console.warn('[Custom Voice] ‚ö†Ô∏è Media stream died unexpectedly');
              attemptMicRecovery(); // Async recovery with retry loop
            }
            return;
          }

          // Check audio context state and resume if needed
          if (audioContextRef.current?.state === 'suspended') {
            audioContextRef.current.resume();
            console.log('[Custom Voice] ‚ö†Ô∏è Resuming suspended audio context');
          }

          const inputData = e.inputBuffer.getChannelData(0);

          // Calculate RMS for VAD
          let sumSquares = 0;
          let maxAmplitude = 0;
          for (let i = 0; i < inputData.length; i++) {
            const amplitude = Math.abs(inputData[i]);
            sumSquares += inputData[i] * inputData[i];
            if (amplitude > maxAmplitude) maxAmplitude = amplitude;
          }
          const rms = Math.sqrt(sumSquares / inputData.length);

          // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          // RESPONSIVE VAD with ECHO PROTECTION for ScriptProcessor fallback
          // - Lowered barge-in thresholds (0.08 RMS, 0.15 peak)
          // - Reduced 300ms cooldown (was 500ms) for snappier response
          // - Debounce timing (150-300ms)
          // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          
          // VAD: detect speech based on RMS or peak amplitude
          const hasAudio = rms > VAD_THRESHOLD || maxAmplitude > 0.02;
          const now = Date.now();
          const timeSincePlayback = now - lastAudioPlaybackStartRef.current;
          
          if (hasAudio && !speechActive) {
            // Skip VAD for 300ms after tutor audio starts (reduced from 500ms)
            if (isTutorSpeakingRef.current && isPlayingRef.current && timeSincePlayback < 300) {
              console.log(`[Custom Voice] ‚è±Ô∏è VAD cooldown active (${(300 - timeSincePlayback).toFixed(0)}ms remaining) - ignoring speech`);
              return;
            }
            
            // If tutor is currently speaking, check for user speech above threshold
            // Lowered from 0.12/0.25 for more responsive interruption
            if (isTutorSpeakingRef.current && isPlayingRef.current) {
              const BARGE_IN_RMS_THRESHOLD = 0.08; // Lowered from 0.12
              const BARGE_IN_PEAK_THRESHOLD = 0.15; // Lowered from 0.25

              if (rms < BARGE_IN_RMS_THRESHOLD || maxAmplitude < BARGE_IN_PEAK_THRESHOLD) {
                console.log(`[Custom Voice] üîá VAD (fallback): Ignoring ambient sound during tutor (rms=${rms.toFixed(4)}, peak=${maxAmplitude.toFixed(4)}) - below barge-in threshold`);
                return;
              }
              
              // Start debounce timer for sustained speech
              if (speechStartTime === 0) {
                speechStartTime = now;
                console.log(`[Custom Voice] üé§ VAD (fallback): Speech onset detected (rms=${rms.toFixed(4)}, starting 150ms debounce...)`);
                return; // Wait for debounce
              }
              
              // Check if speech sustained for 150ms
              if (now - speechStartTime < SPEECH_DEBOUNCE_MS) {
                console.log(`[Custom Voice] ‚è±Ô∏è VAD debounce: ${(now - speechStartTime).toFixed(0)}ms - waiting for sustained speech`);
                return;
              }
              
              // Speech confirmed after debounce - trigger barge-in
              speechActive = true;
              silentChunks = 0;
              speechStartTime = now; // Track speech start for MIN_SPEECH_DURATION check
              console.log(`[Custom Voice] üõë VAD (fallback): CONFIRMED barge-in after debounce (rms=${rms.toFixed(4)}, peak=${maxAmplitude.toFixed(4)})`);
              stopAudio();
              setIsTutorSpeaking(false);
              wsRef.current.send(JSON.stringify({ type: "speech_detected" }));
              
              // POST-INTERRUPTION BUFFER (Dec 10, 2025 FIX)
              // After barge-in, ignore rapid speech-end events for 2 seconds
              // This prevents fragmented transcripts from being sent to AI
              postInterruptionBufferActive = true;
              if (postInterruptionTimeout) clearTimeout(postInterruptionTimeout);
              postInterruptionTimeout = setTimeout(() => {
                postInterruptionBufferActive = false;
                console.log('[Custom Voice] üì¶ Post-interruption buffer ended');
              }, POST_INTERRUPTION_BUFFER_MS);
              console.log('[Custom Voice] üì¶ Post-interruption buffer started (2s)');
            } else {
              // Tutor not speaking - lower threshold OK
              if (speechStartTime === 0) {
                speechStartTime = now;
                console.log("[Custom Voice] üé§ VAD (fallback): Speech onset (tutor not playing)");
                return; // Wait for debounce
              }
              
              if (now - speechStartTime < SPEECH_DEBOUNCE_MS) {
                return;
              }
              
              speechActive = true;
              silentChunks = 0;
              speechStartTime = now; // Track speech start for MIN_SPEECH_DURATION check
              console.log("[Custom Voice] üé§ VAD (fallback): Speech confirmed (tutor not playing)");
            }
          } else if (!hasAudio && speechActive) {
            // POST-INTERRUPTION BUFFER CHECK (Dec 10, 2025 FIX)
            // During buffer period, ignore speech-end events to prevent fragmentation
            if (postInterruptionBufferActive) {
              console.log('[Custom Voice] üì¶ Ignoring silence during post-interruption buffer');
              return;
            }
            
            // Debounce speech end: require 800ms of silence (gives kids more time)
            if (speechEndTime === 0) {
              speechEndTime = now;
              console.log("[Custom Voice] ‚è±Ô∏è VAD (fallback): Silence detected, starting 800ms debounce...");
              return;
            }
            
            if (now - speechEndTime < SILENCE_DEBOUNCE_MS) {
              console.log(`[Custom Voice] ‚è±Ô∏è VAD silence debounce: ${(now - speechEndTime).toFixed(0)}ms`);
              return;
            }
            
            // MINIMUM SPEECH DURATION CHECK (Dec 10, 2025 FIX)
            // Require at least 600ms of speech before considering it complete
            const speechDuration = speechStartTime > 0 ? now - speechStartTime : 0;
            if (speechDuration > 0 && speechDuration < MIN_SPEECH_DURATION_MS) {
              console.log(`[Custom Voice] ‚è±Ô∏è Speech too short (${speechDuration}ms < ${MIN_SPEECH_DURATION_MS}ms), waiting for more...`);
              return;
            }
            
            // SPEECH COALESCING CHECK (Dec 10, 2025 FIX)
            // If we just ended speech less than 1 second ago, this might be continuation
            if (lastSpeechEndTime > 0 && now - lastSpeechEndTime < SPEECH_COALESCE_WINDOW_MS) {
              console.log('[Custom Voice] üì¶ Coalescing rapid speech events - waiting for more');
              return;
            }
            
            // Confirmed silence - CLEANUP ALL STATE (Dec 10, 2025 FIX)
            speechActive = false;
            speechEndTime = 0;
            speechStartTime = 0;
            lastSpeechEndTime = now; // Track for coalescing
            silentChunks = 0;
            // Clear post-interruption buffer on valid speech end
            if (postInterruptionTimeout) {
              clearTimeout(postInterruptionTimeout);
              postInterruptionTimeout = null;
            }
            postInterruptionBufferActive = false;
            console.log("[Custom Voice] üîá VAD (fallback): Speech ended (confirmed)");
          } else if (hasAudio && speechActive) {
            // Reset silence debounce timer if sound detected
            speechEndTime = 0;
            silentChunks = 0;
            // Clear post-interruption buffer if user resumes speaking (Dec 10, 2025 FIX)
            if (postInterruptionBufferActive) {
              postInterruptionBufferActive = false;
              if (postInterruptionTimeout) {
                clearTimeout(postInterruptionTimeout);
                postInterruptionTimeout = null;
              }
              console.log('[Custom Voice] üì¶ Post-interruption buffer cleared (speech resumed)');
            }
          } else if (!hasAudio && !speechActive) {
            // Stay silent, reset timers
            speechStartTime = 0;
            speechEndTime = 0;
          }
          // ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

          // Convert to PCM16 with amplification and SOFT LIMITING
          // Note: We already have 3x hardware gain from GainNode, so use moderate software gain
          const GAIN = 10; // Reduced from 30 to prevent clipping (30x total with 3x hardware)
          const SOFT_THRESHOLD = 0.8; // Start soft limiting at 80% of max amplitude
          
          // Soft limiting function - prevents harsh clipping that breaks Deepgram STT
          // Uses tanh-based compression for values above threshold
          const softLimit = (x: number): number => {
            const absX = Math.abs(x);
            if (absX < SOFT_THRESHOLD) return x;
            const sign = x > 0 ? 1 : -1;
            const excess = absX - SOFT_THRESHOLD;
            const headroom = 1.0 - SOFT_THRESHOLD;
            // Smooth compression: excess is compressed using tanh
            const compressed = SOFT_THRESHOLD + headroom * Math.tanh(excess / headroom * 2);
            return sign * Math.min(compressed, 0.98); // Never quite hit 1.0
          };
          
          const pcm16 = new Int16Array(inputData.length);
          for (let i = 0; i < inputData.length; i++) {
            const amplified = inputData[i] * GAIN;
            const limited = softLimit(amplified);
            pcm16[i] = limited < 0 ? limited * 0x8000 : limited * 0x7FFF;
          }

          const uint8Array = new Uint8Array(pcm16.buffer);
          const binaryString = Array.from(uint8Array).map(byte => String.fromCharCode(byte)).join('');

          wsRef.current.send(JSON.stringify({
            type: "audio",
            data: btoa(binaryString),
          }));
        };

        // Connect: source -> gainNode -> processor -> destination
        source.connect(gainNode);
        gainNode.connect(processor);
        processor.connect(audioContextRef.current.destination);

        console.log("[Custom Voice] üîä Audio chain (fallback): mic -> gain(3x) -> processor -> destination");
        console.log('[Custom Voice] üìä ScriptProcessor connected:', {
          bufferSize: processor.bufferSize,
          inputChannels: processor.numberOfInputs,
          outputChannels: processor.numberOfOutputs,
          contextState: audioContextRef.current.state,
          streamActive: stream.active,
          trackState: stream.getAudioTracks()[0]?.readyState
        });
      }
      
      console.log("[Custom Voice] ‚úÖ Microphone started successfully");
      
    } catch (error: any) {
      console.error("[Custom Voice] ‚ùå Microphone error:", error.name || error.message, error);
      
      let userMessage = '';
      let troubleshooting: string[] = [];
      let errorType = error.name || error.message || 'Unknown';
      
      // Provide specific guidance based on error type
      if (errorType === 'BROWSER_NOT_SUPPORTED' || error.message === 'BROWSER_NOT_SUPPORTED') {
        userMessage = 'üé§ Your browser does not support voice features';
        troubleshooting = [
          'Try using Chrome, Edge, or Firefox',
          'Make sure your browser is up to date',
          'Check that you\'re using HTTPS (secure connection)',
          'You can still chat via text below'
        ];
      } else if (errorType === 'NotAllowedError' || errorType === 'PermissionDeniedError') {
        userMessage = 'üé§ Microphone access was denied';
        troubleshooting = [
          'Click the üîí lock icon in your browser address bar',
          'Change Microphone setting to "Allow"',
          'Refresh the page and start a new session',
          'Or continue using the text chat below'
        ];
      } else if (errorType === 'NotFoundError' || errorType === 'DevicesNotFoundError') {
        userMessage = 'üé§ No microphone detected';
        troubleshooting = [
          'Make sure a microphone is connected to your device',
          'Check your system sound settings',
          'Try a different microphone if available',
          'You can use text chat in the meantime'
        ];
      } else if (errorType === 'NotReadableError' || errorType === 'TrackStartError') {
        userMessage = 'üé§ Microphone is busy or unavailable';
        troubleshooting = [
          'Close other apps using your microphone (Zoom, Teams, Skype, Discord)',
          'Restart your browser',
          'Check system sound settings > Recording devices',
          'For now, you can chat using the text box below'
        ];
      } else if (errorType === 'OverconstrainedError' || errorType === 'ConstraintNotSatisfiedError') {
        userMessage = 'üé§ Microphone settings incompatible';
        troubleshooting = [
          'Your microphone may not support the required audio quality',
          'Try updating your audio drivers',
          'Use text chat while we investigate'
        ];
      } else if (errorType === 'TypeError') {
        userMessage = 'üé§ Browser configuration issue';
        troubleshooting = [
          'Make sure you\'re using HTTPS (secure connection)',
          'Try using a different browser (Chrome, Edge, Firefox)',
          'Update your browser to the latest version',
          'Use text chat as an alternative'
        ];
      } else {
        userMessage = `üé§ Microphone error: ${error.message || 'Unknown error'}`;
        troubleshooting = [
          'Check your browser microphone permissions',
          'Try refreshing the page',
          'Make sure no other apps are using your microphone',
          'You can still chat via text below'
        ];
      }
      
      // Set error state to display to user
      setMicrophoneError({
        message: userMessage,
        troubleshooting: troubleshooting,
        errorType: errorType
      });
      
      // Add friendly message to transcript
      setTranscript(prev => [...prev, {
        speaker: 'system',
        text: `‚ö†Ô∏è ${userMessage}\n\nüí° Don't worry! You can still have a great tutoring session using the text chat box below. Type your questions and your tutor will respond with voice.`,
        timestamp: new Date().toISOString(),
      }]);
      
      // Don't throw - allow session to continue with text-only mode
      console.log('[Custom Voice] üìù Continuing in text-only mode');
    }
  };
  
  const stopMicrophone = () => {
    console.log("[Custom Voice] üõë Stopping microphone...");
    
    // Stop all tracks in the media stream
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => {
        track.stop();
        console.log("[Custom Voice] ‚è∏Ô∏è Stopped track:", track.kind);
      });
      mediaStreamRef.current = null;
    }
    
    // Disconnect and clean up audio processor
    if (processorRef.current) {
      try {
        processorRef.current.disconnect();
        console.log("[Custom Voice] üîå Disconnected audio processor");
      } catch (error) {
        console.warn("[Custom Voice] ‚ö†Ô∏è Error disconnecting processor:", error);
      }
      processorRef.current = null;
    }
    
    console.log("[Custom Voice] ‚úÖ Microphone stopped successfully");
  };
  
  const retryMicrophone = useCallback(async () => {
    console.log("[Custom Voice] üîÑ Retrying microphone access...");
    await forceStartMicrophone(); // Uses force helper to clear recovery flags
  }, []);
  
  const dismissMicrophoneError = useCallback(() => {
    console.log("[Custom Voice] ‚úï Dismissing microphone error");
    
    // Remove system error messages from transcript
    if (microphoneError) {
      setTranscript(prev => prev.filter(
        t => t.speaker !== 'system' || !t.text.includes(microphoneError.message)
      ));
    }
    
    // Clear error state
    setMicrophoneError(null);
  }, [microphoneError]);

  const MAX_AUDIO_QUEUE_SIZE = 20; // Prevent unbounded queue growth
  
  const playAudio = async (base64Audio: string) => {
    if (!audioContextRef.current) {
      audioContextRef.current = new AudioContext({ sampleRate: 16000 });
    }

    try {
      // Safety: Prevent unbounded queue growth if playback stalls
      if (audioQueueRef.current.length >= MAX_AUDIO_QUEUE_SIZE) {
        console.warn(`[Custom Voice] ‚ö†Ô∏è Audio queue at max capacity (${MAX_AUDIO_QUEUE_SIZE}), dropping oldest chunks`);
        audioQueueRef.current = audioQueueRef.current.slice(-10); // Keep last 10 chunks
      }
      
      // Resume audio context if suspended (browser autoplay policy)
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
      }

      const audioData = Uint8Array.from(atob(base64Audio), c => c.charCodeAt(0));
      
      // Convert PCM16 to Float32 for Web Audio API
      const pcm16 = new Int16Array(audioData.buffer);
      const float32 = new Float32Array(pcm16.length);
      for (let i = 0; i < pcm16.length; i++) {
        float32[i] = pcm16[i] / 32768.0;
      }

      const audioBuffer = audioContextRef.current.createBuffer(1, float32.length, 16000);
      audioBuffer.getChannelData(0).set(float32);
      
      // Add to queue and schedule immediately for seamless playback
      audioQueueRef.current.push(audioBuffer);
      
      if (!isPlayingRef.current) {
        // First chunk - start playback chain
        isPlayingRef.current = true;
        nextPlayTimeRef.current = audioContextRef.current.currentTime;
        scheduleNextChunks();
      } else {
        // Already playing - schedule this new chunk seamlessly
        scheduleNextChunks();
      }
      
    } catch (error) {
      console.error("[Custom Voice] ‚ùå Audio playback error:", error);
    }
  };

  const scheduleNextChunks = () => {
    if (!audioContextRef.current) return;
    
    const ctx = audioContextRef.current;
    const CROSSFADE_DURATION = 0.015; // 15ms crossfade between chunks
    
    // Create shared gain node for playback if not exists or if context changed
    // The gain node must belong to the current context
    if (!playbackGainNodeRef.current) {
      playbackGainNodeRef.current = ctx.createGain();
      playbackGainNodeRef.current.connect(ctx.destination);
      console.log("[Custom Voice] üîä Created playback gain node");
    }
    
    // Schedule all queued chunks
    while (audioQueueRef.current.length > 0) {
      const audioBuffer = audioQueueRef.current.shift()!;
      
      // Ensure we're scheduling in the future
      const scheduleTime = Math.max(ctx.currentTime + 0.001, nextPlayTimeRef.current);
      
      const source = ctx.createBufferSource();
      source.buffer = audioBuffer;
      
      // Create individual gain node for crossfade
      const chunkGain = ctx.createGain();
      source.connect(chunkGain);
      chunkGain.connect(playbackGainNodeRef.current);
      
      // Apply subtle crossfade: fade in at start, fade out at end
      const duration = audioBuffer.duration;
      chunkGain.gain.setValueAtTime(0.85, scheduleTime);
      chunkGain.gain.linearRampToValueAtTime(1.0, scheduleTime + Math.min(CROSSFADE_DURATION, duration * 0.1));
      chunkGain.gain.setValueAtTime(1.0, scheduleTime + duration - Math.min(CROSSFADE_DURATION, duration * 0.1));
      chunkGain.gain.linearRampToValueAtTime(0.85, scheduleTime + duration);
      
      // Track this source for cleanup on interruption
      scheduledSourcesRef.current.push(source);
      currentAudioSourceRef.current = source;
      
      // Clean up when this chunk ends
      source.onended = () => {
        // Remove from tracked sources
        const idx = scheduledSourcesRef.current.indexOf(source);
        if (idx > -1) {
          scheduledSourcesRef.current.splice(idx, 1);
        }
        
        // Check if all playback complete
        if (scheduledSourcesRef.current.length === 0 && audioQueueRef.current.length === 0) {
          isPlayingRef.current = false;
          setIsTutorSpeaking(false);
          currentAudioSourceRef.current = null;
        }
      };
      
      // CRITICAL: Start playback at scheduled time (no gaps!)
      try {
        source.start(scheduleTime);
        console.log(`[Custom Voice] üîä Scheduled audio chunk at ${scheduleTime.toFixed(3)}s, duration: ${duration.toFixed(3)}s`);
      } catch (startError) {
        console.error("[Custom Voice] ‚ùå Failed to start audio source:", startError);
      }
      
      // Update next play time: slight overlap for seamless transition
      nextPlayTimeRef.current = scheduleTime + duration - 0.005; // 5ms overlap
    }
  };

  const stopAudio = () => {
    console.log("[Custom Voice] ‚èπÔ∏è Stopping audio playback");
    
    // Stop ALL scheduled audio sources with smooth fadeout
    if (playbackGainNodeRef.current && audioContextRef.current) {
      const now = audioContextRef.current.currentTime;
      // Quick fadeout to avoid clicks
      playbackGainNodeRef.current.gain.cancelScheduledValues(now);
      playbackGainNodeRef.current.gain.setValueAtTime(playbackGainNodeRef.current.gain.value, now);
      playbackGainNodeRef.current.gain.linearRampToValueAtTime(0, now + 0.05); // 50ms fadeout
    }
    
    // Stop all scheduled sources
    scheduledSourcesRef.current.forEach(source => {
      try {
        source.stop();
        source.disconnect();
      } catch (e) {
        // Source might already be stopped
      }
    });
    scheduledSourcesRef.current = [];
    
    // Stop current audio source
    if (currentAudioSourceRef.current) {
      try {
        currentAudioSourceRef.current.stop();
        currentAudioSourceRef.current.disconnect();
      } catch (e) {
        // Source might already be stopped
      }
      currentAudioSourceRef.current = null;
    }
    
    // Clear the audio queue and reset state
    audioQueueRef.current = [];
    isPlayingRef.current = false;
    nextPlayTimeRef.current = 0;
    
    // Reset gain node for next playback
    if (playbackGainNodeRef.current && audioContextRef.current) {
      const now = audioContextRef.current.currentTime;
      playbackGainNodeRef.current.gain.setValueAtTime(1.0, now + 0.06);
    }
    
    console.log("[Custom Voice] ‚úÖ Audio stopped smoothly, microphone still active");
  };

  const cleanup = () => {
    stopAudio();
    
    if (processorRef.current) {
      try {
        processorRef.current.disconnect();
        // Close AudioWorklet port if it exists
        if ('port' in processorRef.current) {
          processorRef.current.port.close();
        }
      } catch (e) {
        console.warn('[Custom Voice] Cleanup warning:', e);
      }
      processorRef.current = null;
    }

    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop());
      mediaStreamRef.current = null;
    }

    // Disconnect playback gain node before closing context
    if (playbackGainNodeRef.current) {
      try {
        playbackGainNodeRef.current.disconnect();
      } catch (e) {
        // Already disconnected
      }
      playbackGainNodeRef.current = null;
    }

    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    audioQueueRef.current = [];
    scheduledSourcesRef.current = [];
    isPlayingRef.current = false;
    nextPlayTimeRef.current = 0;
    
    // Reset stream cleanup flag for next session
    streamCleanupTriggeredRef.current = false;
  };

  const disconnectInProgress = useRef(false);
  
  const disconnect = useCallback(async (sessionId?: string) => {
    // Prevent concurrent disconnect calls
    if (disconnectInProgress.current) {
      console.log("[Custom Voice] ‚ö†Ô∏è Disconnect already in progress, ignoring duplicate call");
      return;
    }
    
    disconnectInProgress.current = true;
    
    console.log("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ");
    console.log("[Custom Voice] üõë DISCONNECT CALLED");
    console.log("[Custom Voice] Session ID:", sessionId);
    console.log("[Custom Voice] WebSocket state:", wsRef.current?.readyState);
    console.log("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ");
    
    // Capture current WebSocket instance to prevent issues if wsRef changes during async ops
    const ws = wsRef.current;
    let ackHandler: ((event: MessageEvent) => void) | null = null;
    let ackReceived = false;
    
    try {
      // Try WebSocket termination if connection is open
      if (ws && ws.readyState === WebSocket.OPEN) {
        console.log("[Custom Voice] ‚úÖ WebSocket is OPEN - attempting WebSocket termination");
        
        let sessionEndedAckReceived = false;
        const HTTP_FALLBACK_TIMEOUT = 3000; // 3 seconds
        
        // Listen for session_ended ACK using addEventListener (doesn't overwrite existing handlers)
        const ackPromise = new Promise<boolean>((resolve) => {
          console.log("[Custom Voice] üïê Setting up ACK listener with", HTTP_FALLBACK_TIMEOUT, "ms timeout");
          
          ackHandler = (event: MessageEvent) => {
            try {
              const message = JSON.parse(event.data);
              console.log("[Custom Voice] üì® Received message during ACK wait:", message.type);
              if (message.type === "session_ended") {
                console.log("[Custom Voice] ‚úÖ Received session_ended ACK - WebSocket succeeded");
                sessionEndedAckReceived = true;
                resolve(true);
              }
            } catch (e) {
              // Ignore parsing errors
            }
          };
          
          // Add our listener (doesn't replace existing onmessage handler)
          console.log("[Custom Voice] üì° Adding ACK event listener");
          ws.addEventListener('message', ackHandler);
          
          // Also listen for close event to resolve early if WebSocket closes
          const closeHandler = () => {
            console.log("[Custom Voice] üîå WebSocket closed before ACK");
            if (!sessionEndedAckReceived) {
              resolve(false);
            }
          };
          ws.addEventListener('close', closeHandler, { once: true });
          
          // Timeout after 3 seconds
          setTimeout(() => {
            console.log("[Custom Voice] ‚è±Ô∏è ACK timeout fired. ACK received?", sessionEndedAckReceived);
            if (!sessionEndedAckReceived) {
              console.log("[Custom Voice] ‚ö†Ô∏è No ACK received within timeout - will use HTTP fallback");
              resolve(false);
            }
          }, HTTP_FALLBACK_TIMEOUT);
        });
      
        console.log("[Custom Voice] üì§ Sending end message via WebSocket...");
        ws.send(JSON.stringify({ type: "end" }));
        console.log("[Custom Voice] ‚è≥ Waiting for ACK or timeout...");
        
        // Wait for ACK or timeout
        ackReceived = await ackPromise;
        console.log("[Custom Voice] üéØ ACK promise resolved. ACK received?", ackReceived);
        
        // Close WebSocket
        console.log("[Custom Voice] üîå Closing WebSocket connection...");
        ws.close(1000, 'User ended session');
        wsRef.current = null;
      } else {
        console.log("[Custom Voice] ‚ö†Ô∏è WebSocket not open or already closed");
        console.log("[Custom Voice] State:", ws?.readyState);
      }
      
      // Always try HTTP fallback if:
      // 1. WebSocket ACK failed (no ACK received)
      // 2. OR WebSocket was not open in the first place (Railway proxy scenario)
      if (!ackReceived && sessionId) {
        console.log("[Custom Voice] üîÑ Using HTTP fallback to end session...");
        console.log("[Custom Voice] üåê HTTP POST to /api/voice-sessions/" + sessionId + "/end");
        try {
          const response = await fetch(`/api/voice-sessions/${sessionId}/end`, {
            method: 'POST',
            credentials: 'include',
          });
          
          console.log("[Custom Voice] üì° HTTP response status:", response.status);
          
          if (response.ok) {
            const result = await response.json();
            console.log("[Custom Voice] ‚úÖ HTTP fallback successful:", result);
          } else {
            const errorText = await response.text();
            console.error("[Custom Voice] ‚ùå HTTP fallback failed:", response.status, errorText);
          }
        } catch (error) {
          console.error("[Custom Voice] ‚ùå HTTP fallback error:", error);
        }
      } else if (!ackReceived && !sessionId) {
        console.warn("[Custom Voice] ‚ö†Ô∏è Cannot end session - no sessionId provided");
      } else {
        console.log("[Custom Voice] ‚úÖ Session ended via WebSocket ACK - HTTP fallback not needed");
      }
      
    } finally {
      // Always cleanup: remove event listener and reset flag
      console.log("[Custom Voice] üßπ Running finally block cleanup");
      
      if (ackHandler && ws) {
        console.log("[Custom Voice] üîÑ Removing ACK listener in finally");
        ws.removeEventListener('message', ackHandler);
      }
      
      cleanup();
      setIsConnected(false);
      disconnectInProgress.current = false;
      
      console.log("[Custom Voice] ‚úÖ Disconnect complete");
      console.log("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ");
    }
    
  }, []);

  const sendTextMessage = useCallback((message: string) => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      console.error("[Custom Voice] Cannot send text message: WebSocket not connected");
      return;
    }

    console.log("[Custom Voice] üìù Sending text message to AI");
    
    // DON'T add to transcript here - let the server send it back to avoid duplicates
    // The WebSocket handler will send back a transcript entry that we'll receive in onmessage

    // Send to WebSocket
    wsRef.current.send(JSON.stringify({
      type: "text_message",
      message: message,
    }));
  }, []);

  const sendDocumentUploaded = useCallback((documentId: string, filename: string) => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      console.error("[Custom Voice] Cannot send document notification: WebSocket not connected");
      return;
    }

    console.log("[Custom Voice] üìÑ Notifying AI about uploaded document:", filename);

    // Send to WebSocket
    wsRef.current.send(JSON.stringify({
      type: "document_uploaded",
      documentId: documentId,
      filename: filename,
    }));
  }, []);

  const updateMode = useCallback(async (tutorAudio: boolean, studentMic: boolean) => {
    console.log("[Custom Voice] üîÑ Updating mode:", { tutorAudio, studentMic });

    const previousMicState = micEnabledRef.current;

    // Update local state (works even before connection for initial setup)
    setAudioEnabled(tutorAudio);
    setMicEnabled(studentMic);

    // Send to server only if connected
    if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
      wsRef.current.send(JSON.stringify({
        type: "update_mode",
        tutorAudio,
        studentMic,
      }));
    } else {
      console.log("[Custom Voice] üìù Mode updated locally (not connected yet)");
    }

    // Stop audio if muting
    if (!tutorAudio && isPlayingRef.current) {
      stopAudio();
    }

    // Handle microphone toggling (only if connected)
    const isConnected = wsRef.current && wsRef.current.readyState === WebSocket.OPEN;
    
    if (isConnected && studentMic && !previousMicState) {
      // Switching to Voice mode - start microphone
      console.log("[Custom Voice] üé§ Enabling microphone for Voice mode");
      await startMicrophone();
    } else if (isConnected && !studentMic && previousMicState) {
      // Switching to Hybrid/Text mode - stop microphone
      console.log("[Custom Voice] üîá Disabling microphone for Hybrid/Text mode");
      stopMicrophone();
    }
  }, []);

  const addSystemMessage = useCallback((message: string) => {
    console.log("[Custom Voice] üì¢ System message:", message);
    setTranscript(prev => [...prev, {
      speaker: "system",
      text: message,
      timestamp: new Date().toISOString(),
    }]);
  }, []);

  return {
    connect,
    disconnect,
    sendTextMessage,
    sendDocumentUploaded,
    updateMode,
    addSystemMessage,
    retryMicrophone,
    dismissMicrophoneError,
    isConnected,
    transcript,
    error,
    microphoneError,
    isTutorSpeaking,
    audioEnabled,
    micEnabled,
  };
}